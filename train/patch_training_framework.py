#!/usr/bin/env python3
"""
Patch-based training framework for PyTorch Lightning.

Features:
- Patch-aware training loops with boundary handling
- Multi-scale loss computation
- Adaptive performance optimization  
- Integrated visualization system
- Progressive training strategies

Compatible with existing FrameInterpolationTrainer architecture.
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import pytorch_lightning as pl
import torchvision.transforms as transforms
import torch.nn.functional as F
import numpy as np
from typing import Dict, Any, Optional, Tuple, List, Union
import logging
from dataclasses import dataclass

# Import existing components
try:
    from training_framework import (
        PerceptualLoss, DistillationLoss, FrameInterpolationTrainer,
        CombinedLoss, EdgeLoss, TemporalConsistencyLoss
    )
except ImportError as e:
    print(f"Training framework import warning: {e}")
    # Provide basic loss function placeholders
    class PerceptualLoss(nn.Module):
        def __init__(self):
            super().__init__()
        def forward(self, pred, target):
            return torch.tensor(0.0)
    
    class DistillationLoss(nn.Module):
        def __init__(self):
            super().__init__()
        def forward(self, student, teacher, target):
            return torch.tensor(0.0), {}
    
    class EdgeLoss(nn.Module):
        def __init__(self):
            super().__init__()
        def forward(self, pred, target):
            return torch.tensor(0.0)
    
    class FrameInterpolationTrainer:
        pass
    class CombinedLoss:
        pass
    class TemporalConsistencyLoss:
        pass

try:
    from training_monitor import TrainingMonitor, create_training_monitor
except ImportError:
    try:
        from train.training_monitor import TrainingMonitor, create_training_monitor
    except ImportError:
        print("Training monitor module import warning")
        class TrainingMonitor:
            def __init__(self, log_dir, config):
                pass
            def log_training_progress(self, progress_dict):
                pass
        def create_training_monitor(config, model=None):
            return TrainingMonitor(config.get('log_dir', './logs'), config)

# Import patch components
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src', 'npu', 'networks'))
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src', 'npu'))

# Use absolute imports to avoid relative import issues
try:
    from src.npu.networks.patch_inpainting import PatchBasedInpainting, PatchInpaintingConfig
    from src.npu.networks.mobile_inpainting_network import MobileInpaintingNetwork
except ImportError:
    # Alternative import method
    sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
    from src.npu.networks.patch_inpainting import PatchBasedInpainting, PatchInpaintingConfig
    from src.npu.networks.mobile_inpainting_network import MobileInpaintingNetwork

# å¯¼å…¥patchæ•°æ®é›†
try:
    from patch_aware_dataset import PatchAwareDataset, PatchTrainingConfig
except ImportError:
    try:
        from train.patch_aware_dataset import PatchAwareDataset, PatchTrainingConfig
    except ImportError:
        print("Patchæ•°æ®é›†å¯¼å…¥è­¦å‘Š")
        class PatchAwareDataset:
            pass
        class PatchTrainingConfig:
            def __init__(self, **kwargs):
                for k, v in kwargs.items():
                    setattr(self, k, v)

# å¯¼å…¥patchå¯è§†åŒ–ç»„ä»¶
try:
    from patch_tensorboard_logger import create_patch_visualizer, PatchTensorBoardLogger
except ImportError:
    try:
        from train.patch_tensorboard_logger import create_patch_visualizer, PatchTensorBoardLogger
    except ImportError:
        print("Patch TensorBoardå¯è§†åŒ–å¯¼å…¥è­¦å‘Š")
        def create_patch_visualizer(log_dir, config=None):
            return None
        class PatchTensorBoardLogger:
            def __init__(self, *args, **kwargs):
                pass
            def log_patch_visualization(self, *args, **kwargs):
                pass
            def log_patch_comparison(self, *args, **kwargs):
                pass
            def log_training_step(self, *args, **kwargs):
                pass
            def log_validation_step(self, *args, **kwargs):
                pass


@dataclass
class PatchTrainingScheduleConfig:
    """Patchè®­ç»ƒè°ƒåº¦é…ç½®"""
    # è®­ç»ƒé˜¶æ®µé…ç½®
    patch_warmup_epochs: int = 20           # patchæ¨¡å¼çƒ­èº«é˜¶æ®µ
    mixed_training_epochs: int = 50         # æ··åˆè®­ç»ƒé˜¶æ®µ
    full_fine_tuning_epochs: int = 30       # å…¨å›¾fine-tuningé˜¶æ®µ
    
    # æ¨¡å¼åˆ‡æ¢ç­–ç•¥
    initial_patch_probability: float = 0.9   # åˆå§‹patchæ¦‚ç‡
    final_patch_probability: float = 0.3     # æœ€ç»ˆpatchæ¦‚ç‡
    
    # è‡ªé€‚åº”è°ƒåº¦
    enable_adaptive_scheduling: bool = True  # å¯ç”¨è‡ªé€‚åº”è°ƒåº¦
    loss_patience: int = 5                   # æŸå¤±æ— æ”¹å–„å®¹å¿epochs
    
    # æ€§èƒ½é˜ˆå€¼
    patch_efficiency_threshold: float = 1.5  # patchæ•ˆç‡é˜ˆå€¼ï¼ˆç›¸æ¯”fullæ¨¡å¼ï¼‰
    quality_drop_threshold: float = 0.05     # è´¨é‡ä¸‹é™å®¹å¿é˜ˆå€¼


class PatchAwareLoss(nn.Module):
    """
    Patch-AwareæŸå¤±å‡½æ•°ç³»ç»Ÿ
    
    åŠŸèƒ½ï¼š
    1. Multi-scaleæŸå¤±ï¼špatch-level + image-level
    2. Boundary-awareå¤„ç†ï¼špatchè¾¹ç•Œçš„ç‰¹æ®ŠæŸå¤±
    3. ConsistencyæŸå¤±ï¼špatchèåˆä¸€è‡´æ€§
    4. è‡ªé€‚åº”æƒé‡ï¼šåŸºäºè®­ç»ƒé˜¶æ®µçš„åŠ¨æ€æƒé‡è°ƒæ•´
    """
    
    def __init__(self, 
                 lambda_patch: float = 1.0,
                 lambda_full: float = 1.0,
                 lambda_boundary: float = 0.5,
                 lambda_consistency: float = 0.3,
                 enable_adaptive_weights: bool = True):
        super(PatchAwareLoss, self).__init__()
        
        # æŸå¤±æƒé‡
        self.lambda_patch = lambda_patch
        self.lambda_full = lambda_full
        self.lambda_boundary = lambda_boundary
        self.lambda_consistency = lambda_consistency
        self.enable_adaptive_weights = enable_adaptive_weights
        
        # åŸºç¡€æŸå¤±å‡½æ•°
        self.l1_loss = nn.L1Loss()
        self.mse_loss = nn.MSELoss()
        self.perceptual_loss = PerceptualLoss()
        self.edge_loss = EdgeLoss()
        
        # patchä¸“ç”¨æŸå¤±
        self.boundary_kernel = self._create_boundary_kernel()
        
    def _create_boundary_kernel(self) -> torch.Tensor:
        """åˆ›å»ºè¾¹ç•Œæ£€æµ‹å·ç§¯æ ¸"""
        kernel = torch.tensor([
            [-1, -1, -1],
            [-1,  8, -1],
            [-1, -1, -1]
        ], dtype=torch.float32).unsqueeze(0).unsqueeze(0)
        return kernel
    
    def forward(self, 
                predictions: Dict[str, torch.Tensor],
                targets: Dict[str, torch.Tensor],
                mode: str,
                epoch: int = 0,
                metadata: Optional[Dict] = None) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        """
        è®¡ç®—patch-awareæŸå¤±
        
        Args:
            predictions: é¢„æµ‹ç»“æœå­—å…¸
            targets: ç›®æ ‡ç»“æœå­—å…¸
            mode: 'patch' (ç°åœ¨åªæ”¯æŒpatchæ¨¡å¼)
            epoch: å½“å‰epochï¼ˆç”¨äºè‡ªé€‚åº”æƒé‡ï¼‰
            metadata: é¢å¤–å…ƒæ•°æ®
            
        Returns:
            total_loss: æ€»æŸå¤±
            loss_dict: åˆ†é¡¹æŸå¤±å­—å…¸
        """
        losses = {}
        total_loss = 0.0
        
        # è®¡ç®—è‡ªé€‚åº”æƒé‡
        if self.enable_adaptive_weights:
            weights = self._compute_adaptive_weights(epoch, mode)
        else:
            weights = {
                'patch': self.lambda_patch,
                'full': self.lambda_full,
                'boundary': self.lambda_boundary,
                'consistency': self.lambda_consistency
            }
        
        # Patchæ¨¡å¼æŸå¤±
        if 'patch' in predictions:
            patch_pred = predictions['patch']  # [N, 3, 128, 128]
            patch_target = targets['patch']    # [N, 3, 128, 128]
            
            # åŸºç¡€patchæŸå¤±
            patch_l1 = self.l1_loss(patch_pred, patch_target)
            patch_perceptual = self.perceptual_loss(patch_pred, patch_target)
            patch_edge = self.edge_loss(patch_pred, patch_target)
            
            # è¾¹ç•Œæ„ŸçŸ¥æŸå¤±
            boundary_loss = self._compute_boundary_loss(
                patch_pred, patch_target, metadata.get('patch_metadata', []) if metadata else []
            )
            
            losses['patch_l1'] = patch_l1
            losses['patch_perceptual'] = patch_perceptual
            losses['patch_edge'] = patch_edge
            losses['patch_boundary'] = boundary_loss
            
            # patchæ€»æŸå¤± - ğŸ”§ å¢å¼ºæ„ŸçŸ¥æŸå¤±å’Œè¾¹ç¼˜æŸå¤±æƒé‡
            patch_total = (patch_l1 + 0.8 * patch_perceptual + 
                          0.5 * patch_edge + weights['boundary'] * boundary_loss)
            total_loss += weights['patch'] * patch_total
        
        # æ³¨æ„ï¼šåˆ é™¤äº†Fullæ¨¡å¼æŸå¤±å’Œä¸€è‡´æ€§æŸå¤±ï¼Œç°åœ¨åªä¸“æ³¨äºpatchè®­ç»ƒ
        
        losses['total'] = total_loss
        return total_loss, losses
    
    def _compute_adaptive_weights(self, epoch: int, mode: str) -> Dict[str, float]:
        """è®¡ç®—è‡ªé€‚åº”æŸå¤±æƒé‡ - ç®€åŒ–ä¸ºåªæ”¯æŒpatchæ¨¡å¼"""
        # åŸºäºè®­ç»ƒè¿›åº¦è°ƒæ•´æƒé‡
        progress = min(epoch / 100.0, 1.0)  # å‡è®¾100 epochä¸ºå®Œæ•´è®­ç»ƒ
        
        # ç°åœ¨åªæœ‰patchæ¨¡å¼ï¼šearlyé‡patchï¼Œlateré‡boundary
        patch_weight = 1.0
        boundary_weight = 0.5 - 0.2 * progress  # è®­ç»ƒåæœŸå‡å°‘è¾¹ç•Œæƒé‡
        
        return {
            'patch': patch_weight,
            'boundary': boundary_weight,
            'consistency': 0.0,  # ä¸å†ä½¿ç”¨ä¸€è‡´æ€§æŸå¤±
            'full': 0.0  # ä¸å†ä½¿ç”¨å…¨å›¾æŸå¤±
        }
    
    def _compute_boundary_loss(self, pred: torch.Tensor, target: torch.Tensor, 
                              patch_metadata: List[Dict]) -> torch.Tensor:
        """è®¡ç®—è¾¹ç•Œæ„ŸçŸ¥æŸå¤±"""
        if len(patch_metadata) == 0:
            return torch.tensor(0.0, device=pred.device)
        
        # ç®€åŒ–ç‰ˆè¾¹ç•ŒæŸå¤±ï¼šåœ¨patchè¾¹ç•ŒåŒºåŸŸåŠ æƒ
        boundary_loss = 0.0
        
        for i, metadata in enumerate(patch_metadata):
            if i >= pred.shape[0]:
                break
                
            # æ£€æµ‹è¾¹ç•ŒåŒºåŸŸï¼ˆç®€å•çš„è¾¹ç¼˜æ£€æµ‹ï¼‰
            if self.boundary_kernel.device != pred.device:
                self.boundary_kernel = self.boundary_kernel.to(pred.device)
            
            # å¯¹æ¯ä¸ªé€šé“åˆ†åˆ«å¤„ç†
            patch_pred = pred[i:i+1]  # [1, 3, 128, 128]
            patch_target = target[i:i+1]  # [1, 3, 128, 128]
            
            # è¾¹ç•Œæ£€æµ‹ï¼ˆä½¿ç”¨ç°åº¦åŒ–åçš„ç»“æœï¼‰
            pred_gray = torch.mean(patch_pred, dim=1, keepdim=True)  # [1, 1, 128, 128]
            boundary_map = F.conv2d(pred_gray, self.boundary_kernel, padding=1)
            boundary_map = torch.sigmoid(boundary_map * 0.1)
            
            # åœ¨è¾¹ç•ŒåŒºåŸŸåŠ æƒL1æŸå¤±
            boundary_weighted_loss = torch.mean(
                F.l1_loss(patch_pred, patch_target, reduction='none') * 
                (1.0 + boundary_map)  # è¾¹ç•ŒåŒºåŸŸæƒé‡å¢å¼º
            )
            boundary_loss += boundary_weighted_loss
        
        return boundary_loss / len(patch_metadata) if patch_metadata else torch.tensor(0.0, device=pred.device)
    


class PatchTrainingScheduler:
    """
    Patchè®­ç»ƒè°ƒåº¦å™¨
    
    åŠŸèƒ½ï¼š
    1. è®­ç»ƒé˜¶æ®µç®¡ç†
    2. æ¨¡å¼æ¦‚ç‡åŠ¨æ€è°ƒæ•´
    3. è‡ªé€‚åº”æ€§èƒ½ä¼˜åŒ–
    4. æ—©åœå’Œè´¨é‡ç›‘æ§
    """
    
    def __init__(self, config: PatchTrainingScheduleConfig):
        self.config = config
        self.current_epoch = 0
        self.patch_performance_history = []
        self.full_performance_history = []
        
    def get_patch_probability(self, epoch: int, recent_losses: Optional[List[float]] = None) -> float:
        """è·å–å½“å‰epochåº”è¯¥ä½¿ç”¨çš„patchæ¨¡å¼æ¦‚ç‡"""
        self.current_epoch = epoch
        
        # åŸºç¡€è°ƒåº¦ç­–ç•¥
        if epoch < self.config.patch_warmup_epochs:
            # çƒ­èº«é˜¶æ®µï¼šé«˜patchæ¦‚ç‡
            base_prob = self.config.initial_patch_probability
        elif epoch < self.config.patch_warmup_epochs + self.config.mixed_training_epochs:
            # æ··åˆè®­ç»ƒé˜¶æ®µï¼šçº¿æ€§ä¸‹é™
            progress = (epoch - self.config.patch_warmup_epochs) / self.config.mixed_training_epochs
            base_prob = (self.config.initial_patch_probability - 
                        (self.config.initial_patch_probability - self.config.final_patch_probability) * progress)
        else:
            # Fine-tuningé˜¶æ®µï¼šä½patchæ¦‚ç‡
            base_prob = self.config.final_patch_probability
        
        # è‡ªé€‚åº”è°ƒæ•´
        if self.config.enable_adaptive_scheduling and recent_losses:
            base_prob = self._adaptive_adjustment(base_prob, recent_losses)
        
        return np.clip(base_prob, 0.1, 0.9)
    
    def _adaptive_adjustment(self, base_prob: float, recent_losses: List[float]) -> float:
        """åŸºäºæœ€è¿‘æŸå¤±å†å²çš„è‡ªé€‚åº”è°ƒæ•´"""
        if len(recent_losses) < 3:
            return base_prob
        
        # æ£€æŸ¥æŸå¤±è¶‹åŠ¿
        recent_trend = recent_losses[-1] - recent_losses[-3]
        
        if recent_trend > 0:  # æŸå¤±ä¸Šå‡
            # å¢åŠ patchæ¦‚ç‡ï¼ˆpatchè®­ç»ƒé€šå¸¸æ›´ç¨³å®šï¼‰
            adjustment = 0.1
        else:  # æŸå¤±ä¸‹é™
            # ä¿æŒå½“å‰ç­–ç•¥
            adjustment = 0.0
        
        return base_prob + adjustment
    
    def should_switch_strategy(self, current_metrics: Dict[str, float]) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åˆ‡æ¢è®­ç»ƒç­–ç•¥"""
        if not self.config.enable_adaptive_scheduling:
            return False
        
        # åŸºäºæ€§èƒ½æŒ‡æ ‡å†³å®šç­–ç•¥åˆ‡æ¢
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„é€»è¾‘
        return False


class PatchFrameInterpolationTrainer(pl.LightningModule):
    """
    Patch-Basedå¸§æ’å€¼è®­ç»ƒå™¨
    
    ç»§æ‰¿è‡ªPyTorch Lightningï¼Œæ”¯æŒpatch-awareè®­ç»ƒï¼š
    1. æ™ºèƒ½æ¨¡å¼è°ƒåº¦
    2. åŠ¨æ€batchå¤„ç†  
    3. å¤šå°ºåº¦æŸå¤±ä¼˜åŒ–
    4. è‡ªé€‚åº”æ€§èƒ½è°ƒä¼˜
    5. å®Œæ•´çš„ç›‘æ§å’Œæ—¥å¿—
    """
    
    def __init__(self,
                 model_config: Dict[str, Any],
                 training_config: Dict[str, Any],
                 patch_config: Optional[PatchTrainingConfig] = None,
                 schedule_config: Optional[PatchTrainingScheduleConfig] = None,
                 teacher_model_path: Optional[str] = None,
                 full_config: Optional[Dict[str, Any]] = None):
        super(PatchFrameInterpolationTrainer, self).__init__()
        
        self.save_hyperparameters()
        
        # é…ç½®
        self.model_config = model_config
        self.training_config = training_config
        self.patch_config = patch_config or PatchTrainingConfig()
        self.schedule_config = schedule_config or PatchTrainingScheduleConfig()
        
        # åˆ›å»ºpatch-basedç½‘ç»œ
        inpainting_config = model_config.get('inpainting_network', {})
        patch_inpainting_config = PatchInpaintingConfig(
            enable_patch_mode=self.patch_config.enable_patch_mode,
            patch_network_channels=24  # è½»é‡åŒ–é…ç½®
        )
        
        self.student_model = PatchBasedInpainting(
            input_channels=inpainting_config.get('input_channels', 7),
            output_channels=inpainting_config.get('output_channels', 3),
            config=patch_inpainting_config
        )
        
        # æ³¨æ„ï¼šåˆ é™¤äº†fallbackå…¨å›¾ç½‘ç»œï¼Œç°åœ¨åªä¸“æ³¨patchè®­ç»ƒ
        
        # åŠ è½½æ•™å¸ˆç½‘ç»œï¼ˆçŸ¥è¯†è’¸é¦ï¼‰
        self.teacher_model = None
        if teacher_model_path and os.path.exists(teacher_model_path):
            self.teacher_model = MobileInpaintingNetwork(
                input_channels=inpainting_config.get('input_channels', 7),
                output_channels=inpainting_config.get('output_channels', 3)
            )
            checkpoint = torch.load(teacher_model_path, map_location='cpu')
            if 'state_dict' in checkpoint:
                self.teacher_model.load_state_dict(checkpoint['state_dict'])
            else:
                self.teacher_model.load_state_dict(checkpoint)
            self.teacher_model.eval()
            for param in self.teacher_model.parameters():
                param.requires_grad = False
        
        # æŸå¤±å‡½æ•°
        self.patch_loss = PatchAwareLoss()
        
        if self.teacher_model is not None:
            self.distillation_loss = DistillationLoss()
        
        # è®­ç»ƒè°ƒåº¦å™¨
        self.scheduler = PatchTrainingScheduler(self.schedule_config)
        
        # æ€§èƒ½ç›‘æ§
        monitor_config = (full_config or {}).copy()
        monitor_config['log_dir'] = training_config.get('log_dir', './logs')
        self.training_monitor = create_training_monitor(
            config=monitor_config,
            model=self.student_model
        )
        
        # Patchä¸“ç”¨å¯è§†åŒ–ç³»ç»Ÿ
        viz_config = {
            'visualization_frequency': full_config.get('visualization', {}).get('visualization_frequency', 100),
            'save_frequency': full_config.get('visualization', {}).get('save_frequency', 500),
            'enable_visualization': full_config.get('visualization', {}).get('enable_visualization', True)
        }
        self.patch_visualizer = create_patch_visualizer(
            log_dir=training_config.get('log_dir', './logs'),
            config=viz_config
        ) if viz_config['enable_visualization'] else None
        
        # è®­ç»ƒç»Ÿè®¡
        self.training_stats = {
            'patch_steps': 0,
            'recent_losses': [],
            'best_val_loss': float('inf'),
            'patience_count': 0
        }
        
        # å¯è§†åŒ–æ­¥éª¤è®¡æ•°å™¨ï¼ˆé¿å…ä¸Lightningçš„global_stepå†²çªï¼‰
        self.visualization_step = 0
        
        print(f"Patch visualization system: {'ENABLED' if self.patch_visualizer else 'DISABLED'}")
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        return self.student_model(x)
    
    def training_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> Dict[str, torch.Tensor]:
        """è®­ç»ƒæ­¥éª¤"""
        current_epoch = self.current_epoch
        
        # è·å–å½“å‰çš„patchæ¦‚ç‡
        patch_probability = self.scheduler.get_patch_probability(
            current_epoch, 
            self.training_stats['recent_losses'][-10:] if self.training_stats['recent_losses'] else None
        )
        
        # åŠ¨æ€è®¾ç½®patchæ¨¡å¼æ¦‚ç‡
        self.student_model.config.enable_patch_mode = True
        
        # å‡†å¤‡è¾“å…¥æ•°æ®
        predictions = {}
        targets = {}
        
        # å¤„ç†patchæ•°æ®ï¼ˆç°åœ¨åªæœ‰patchæ•°æ®ï¼‰
        if 'patch_input' in batch and len(batch['patch_input']) > 0:
            patch_input = batch['patch_input']    # [N, 7, 128, 128]
            patch_target = batch['patch_target']  # [N, 3, 128, 128]
            
            # Patchç½‘ç»œæ¨ç†
            patch_pred = self.student_model.patch_network(patch_input)
            predictions['patch'] = patch_pred
            targets['patch'] = patch_target
            
            self.training_stats['patch_steps'] += 1
        else:
            # å¦‚æœæ²¡æœ‰patchæ•°æ®ï¼Œè·³è¿‡è¿™ä¸ªbatch
            return {'loss': torch.tensor(0.0, requires_grad=True)}
        
        # ç°åœ¨åªæœ‰patchæ¨¡å¼
        mode = 'patch'
        
        # è®¡ç®—æŸå¤±
        loss, loss_dict = self.patch_loss(
            predictions, targets, mode, current_epoch,
            metadata=batch.get('batch_info', {})
        )
        
        # æ³¨æ„ï¼šçŸ¥è¯†è’¸é¦æš‚æ—¶ç¦ç”¨ï¼Œå› ä¸ºåˆ é™¤äº†å…¨å›¾æ¨¡å¼
        # TODO: å¦‚éœ€çŸ¥è¯†è’¸é¦ï¼Œéœ€è¦å®ç°patch-levelçš„è’¸é¦ç­–ç•¥
        
        # æ›´æ–°ç»Ÿè®¡
        self.training_stats['recent_losses'].append(loss.item())
        if len(self.training_stats['recent_losses']) > 20:
            self.training_stats['recent_losses'].pop(0)
        
        # æ—¥å¿—è®°å½•
        self.log_dict({
            'train_loss': loss,
            'patch_probability': patch_probability,
            'mode': 0,  # ç°åœ¨åªæœ‰patchæ¨¡å¼
            **{f'train_{k}': v for k, v in loss_dict.items() if k != 'total'}
        }, on_step=True, on_epoch=True, prog_bar=True)
        
        # Patchå¯è§†åŒ–è®°å½•ï¼ˆä½¿ç”¨Lightningçš„global_stepï¼‰
        current_global_step = self.global_step if hasattr(self, 'trainer') and self.trainer else self.visualization_step
        
        if self.patch_visualizer and self.patch_visualizer.should_visualize(current_global_step):
            try:
                # è®°å½•patchå¯¹æ¯”ï¼ˆè¾“å…¥|ç›®æ ‡|é¢„æµ‹ï¼‰
                if 'patch' in predictions and 'patch' in targets:
                    self.patch_visualizer.log_patch_comparison(
                        step=current_global_step,
                        patch_inputs=batch['patch_input'][:8],  # æœ€å¤šæ˜¾ç¤º8ä¸ªpatches
                        patch_targets=targets['patch'][:8],
                        patch_predictions=predictions['patch'][:8],
                        tag=f'training_epoch_{current_epoch}'
                    )
                
                # è®°å½•è®­ç»ƒæ­¥éª¤ç»Ÿè®¡
                self.patch_visualizer.log_training_step(
                    step=current_global_step,
                    epoch=current_epoch,
                    mode=mode,
                    loss_dict=loss_dict,
                    batch_info=batch.get('batch_info', {}),
                    learning_rate=self.trainer.optimizers[0].param_groups[0]['lr'] if self.trainer.optimizers else None
                )
                
            except Exception as e:
                print(f"WARNING: Visualization logging failed: {e}")
        
        # æ›´æ–°å¯è§†åŒ–æ­¥éª¤è®¡æ•°ï¼ˆä»…åœ¨æ²¡æœ‰traineræ—¶ä½¿ç”¨ï¼‰
        if not hasattr(self, 'trainer') or not self.trainer:
            self.visualization_step += 1
        
        return {
            'loss': loss,
            'mode': mode,
            'batch_info': batch.get('batch_info', {}),
            'predictions': predictions,
            'targets': targets
        }
    
    def validation_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> Dict[str, torch.Tensor]:
        """éªŒè¯æ­¥éª¤"""
        # éªŒè¯æ—¶ä½¿ç”¨æ··åˆæ¨¡å¼ï¼Œpatchæ¦‚ç‡å›ºå®šä¸º0.5
        predictions = {}
        targets = {}
        
        # å¤„ç†éªŒè¯æ•°æ®ï¼ˆä¸è®­ç»ƒæ­¥éª¤ç±»ä¼¼ï¼Œä½†æ— æ¢¯åº¦æ›´æ–°ï¼‰
        if 'patch_input' in batch and len(batch['patch_input']) > 0:
            patch_pred = self.student_model.patch_network(batch['patch_input'])
            predictions['patch'] = patch_pred
            targets['patch'] = batch['patch_target']
        
        # ç°åœ¨åªæœ‰patchæ¨¡å¼
        mode = 'patch'
        
        # è®¡ç®—éªŒè¯æŸå¤±
        val_loss, val_loss_dict = self.patch_loss(
            predictions, targets, mode, self.current_epoch
        )
        
        # æ—¥å¿—è®°å½•
        self.log_dict({
            'val_loss': val_loss,
            **{f'val_{k}': v for k, v in val_loss_dict.items() if k != 'total'}
        }, on_step=False, on_epoch=True, prog_bar=True)
        
        # éªŒè¯å¯è§†åŒ–è®°å½•
        current_global_step = self.global_step if hasattr(self, 'trainer') and self.trainer else self.visualization_step
        
        if self.patch_visualizer and self.current_epoch % 5 == 0:  # æ¯5ä¸ªepochè®°å½•éªŒè¯å›¾åƒ
            try:
                if 'patch' in predictions and 'patch' in targets:
                    self.patch_visualizer.log_patch_comparison(
                        step=current_global_step,
                        patch_inputs=batch['patch_input'][:4],  # éªŒè¯æ—¶æ˜¾ç¤º4ä¸ªpatches
                        patch_targets=targets['patch'][:4],
                        patch_predictions=predictions['patch'][:4],
                        tag=f'validation_epoch_{self.current_epoch}'
                    )
                
                # è®°å½•éªŒè¯æŸå¤±
                self.patch_visualizer.log_validation_step(
                    step=current_global_step,
                    val_loss_dict=val_loss_dict
                )
                
            except Exception as e:
                print(f"WARNING: Validation visualization logging failed: {e}")
        
        return {
            'val_loss': val_loss,
            'mode': mode,
            'predictions': predictions,
            'targets': targets
        }
    
    def configure_optimizers(self) -> Dict[str, Any]:
        """é…ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        # ä¼˜åŒ–å™¨é…ç½®
        optimizer_config = self.training_config.get('optimizer', {})
        optimizer = optim.AdamW(
            self.student_model.parameters(),
            lr=optimizer_config.get('learning_rate', 1e-4),
            weight_decay=optimizer_config.get('weight_decay', 1e-5),
            betas=(0.9, 0.999)
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler_config = self.training_config.get('scheduler', {})
        scheduler = optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=scheduler_config.get('T_max', 100),
            eta_min=scheduler_config.get('eta_min', 1e-6)
        )
        
        return {
            'optimizer': optimizer,
            'lr_scheduler': {
                'scheduler': scheduler,
                'monitor': 'val_loss',
                'interval': 'epoch',
                'frequency': 1
            }
        }
    
    def on_epoch_end(self) -> None:
        """Epochç»“æŸæ—¶çš„å¤„ç†"""
        # æ›´æ–°è®­ç»ƒç›‘æ§
        if hasattr(self, 'training_monitor'):
            self.training_monitor.log_training_progress({
                'epoch': self.current_epoch,
                'patch_steps': self.training_stats['patch_steps']
            })
        
        # Patchè®­ç»ƒè¿›åº¦å¯è§†åŒ–
        if self.patch_visualizer and self.current_epoch % 10 == 0:  # æ¯10ä¸ªepochè®°å½•è®­ç»ƒè¿›åº¦
            try:
                current_global_step = self.global_step if hasattr(self, 'trainer') and self.trainer else self.visualization_step
                patch_stats = self.get_training_stats()
                self.patch_visualizer.log_training_progress(
                    step=current_global_step,
                    epoch=self.current_epoch,
                    patch_stats=patch_stats
                )
            except Exception as e:
                print(f"WARNING: Training progress visualization failed: {e}")
        
        # è‡ªé€‚åº”è°ƒæ•´
        current_val_loss = self.trainer.callback_metrics.get('val_loss', float('inf'))
        if current_val_loss < self.training_stats['best_val_loss']:
            self.training_stats['best_val_loss'] = current_val_loss
            self.training_stats['patience_count'] = 0
        else:
            self.training_stats['patience_count'] += 1
    
    def get_training_stats(self) -> Dict[str, Any]:
        """è·å–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        total_steps = self.training_stats['patch_steps']
        
        # è·å–patchç½‘ç»œæ€§èƒ½ç»Ÿè®¡
        patch_network_stats = {}
        if hasattr(self.student_model, 'get_performance_stats'):
            try:
                patch_network_stats = self.student_model.get_performance_stats()
            except:
                pass
        
        return {
            **self.training_stats,
            'total_steps': total_steps,
            'patch_ratio': 1.0,  # ç°åœ¨åªæœ‰patchæ¨¡å¼
            'patch_mode_count': self.training_stats['patch_steps'],
            'avg_patches_per_image': patch_network_stats.get('avg_patches_per_image', 0),
            'total_patches_generated': patch_network_stats.get('total_patches_processed', 0),
            'cache_hit_rate': 0.0,  # å ä½ç¬¦ï¼Œå®é™…éœ€è¦ä»æ•°æ®é›†è·å–
            'processing_speed': 100.0  # å ä½ç¬¦
        }


def create_patch_trainer(model_config: Dict[str, Any],
                        training_config: Dict[str, Any],
                        patch_config: Optional[PatchTrainingConfig] = None,
                        schedule_config: Optional[PatchTrainingScheduleConfig] = None,
                        teacher_model_path: Optional[str] = None,
                        full_config: Optional[Dict[str, Any]] = None) -> PatchFrameInterpolationTrainer:
    """åˆ›å»ºPatchè®­ç»ƒå™¨çš„å·¥å‚å‡½æ•°"""
    
    trainer = PatchFrameInterpolationTrainer(
        model_config=model_config,
        training_config=training_config,
        patch_config=patch_config,
        schedule_config=schedule_config,
        teacher_model_path=teacher_model_path,
        full_config=full_config
    )
    
    # å¦‚æœtraineræœ‰å¯è§†åŒ–ç³»ç»Ÿï¼Œè¾“å‡ºå¯ç”¨çŠ¶æ€
    if hasattr(trainer, 'patch_visualizer') and trainer.patch_visualizer:
        print("SUCCESS: Patch visualization system integrated into training framework")
        print(f"   - å¯è§†åŒ–é¢‘ç‡: æ¯{trainer.patch_visualizer.vis_frequency}æ­¥")
        print(f"   - ä¿å­˜é¢‘ç‡: æ¯{trainer.patch_visualizer.save_frequency}æ­¥")
        print(f"   - æ—¥å¿—ç›®å½•: {trainer.patch_visualizer.log_dir}")
    
    return trainer


def test_patch_training_framework():
    """æµ‹è¯•Patchè®­ç»ƒæ¡†æ¶"""
    # é…ç½®
    model_config = {
        'inpainting_network': {
            'input_channels': 7,
            'output_channels': 3
        }
    }
    
    training_config = {
        'optimizer': {
            'learning_rate': 1e-4,
            'weight_decay': 1e-5
        },
        'scheduler': {
            'T_max': 100,
            'eta_min': 1e-6
        },
        'log_dir': './logs'
    }
    
    # åˆ›å»ºè®­ç»ƒå™¨
    try:
        trainer = create_patch_trainer(
            model_config=model_config,
            training_config=training_config
        )
        
        print("SUCCESS: PatchFrameInterpolationTrainer created successfully")
        print(f"Patchç½‘ç»œå‚æ•°: {sum(p.numel() for p in trainer.student_model.parameters()):,}")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        test_input = torch.randn(1, 7, 270, 480)
        with torch.no_grad():
            output = trainer(test_input)
        print(f"å‰å‘ä¼ æ’­æµ‹è¯•: {test_input.shape} -> {output.shape}")
        
        return True
        
    except Exception as e:
        print(f"ERROR: Test failed: {e}")
        return False


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    success = test_patch_training_framework()
    print(f"\n{'SUCCESS: Patch training framework test passed!' if success else 'ERROR: Patch training framework test failed!'}")