#!/usr/bin/env python3
"""
@file train_knowledge_distillation.py
@brief çŸ¥è¯†è’¸é¦ä¸“ç”¨è®­ç»ƒè„šæœ¬

åŠŸèƒ½æè¿°ï¼š
- ä¸“é—¨ç”¨äºTeacher-StudentçŸ¥è¯†è’¸é¦è®­ç»ƒ
- æ”¯æŒå¤šé˜¶æ®µè’¸é¦ï¼šç‰¹å¾è’¸é¦ + è¾“å‡ºè’¸é¦
- æ¸è¿›å¼è’¸é¦ç­–ç•¥å’Œæ¸©åº¦è°ƒåº¦
- è’¸é¦æ•ˆæœè¯„ä¼°å’Œå¯¹æ¯”åˆ†æ

è’¸é¦æµç¨‹ï¼š
1. åŠ è½½é¢„è®­ç»ƒçš„Teacheræ¨¡å‹
2. åˆå§‹åŒ–è½»é‡çº§Studentæ¨¡å‹
3. è®¾è®¡è’¸é¦æŸå¤±å‡½æ•°ç»„åˆ
4. æ‰§è¡Œæ¸è¿›å¼è’¸é¦è®­ç»ƒ
5. è¯„ä¼°è’¸é¦æ•ˆæœå’Œæ€§èƒ½å¯¹æ¯”

æŠ€æœ¯ç‰¹ç‚¹ï¼š
- å¤šå±‚æ¬¡ç‰¹å¾è’¸é¦
- è‡ªé€‚åº”æ¸©åº¦è°ƒåº¦
- è’¸é¦-å¾®è°ƒè”åˆä¼˜åŒ–
- æ€§èƒ½ä¿æŒç‡å®æ—¶ç›‘æ§

@author AIç®—æ³•å›¢é˜Ÿ
@date 2025-07-28
@version 1.0
"""

import os
import sys
import argparse
import yaml
import time
from pathlib import Path
from typing import Dict, Any, Optional, Tuple
import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
from pytorch_lightning.loggers import TensorBoardLogger

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src', 'npu', 'networks'))

from training_framework import FrameInterpolationTrainer
from mobile_inpainting_network import MobileInpaintingNetwork
from train import create_multi_game_dataset, load_config, set_random_seeds


class KnowledgeDistillationTrainer(pl.LightningModule):
    """
    çŸ¥è¯†è’¸é¦è®­ç»ƒå™¨
    
    ä¸“é—¨ç”¨äºTeacher-Studentæ¡†æ¶çš„è’¸é¦è®­ç»ƒ
    """
    
    def __init__(self,
                 teacher_model_path: str,
                 student_config: Dict[str, Any],
                 distillation_config: Dict[str, Any],
                 training_config: Dict[str, Any]):
        """
        åˆå§‹åŒ–çŸ¥è¯†è’¸é¦è®­ç»ƒå™¨
        
        Args:
            teacher_model_path: Teacheræ¨¡å‹è·¯å¾„
            student_config: Studentæ¨¡å‹é…ç½®
            distillation_config: è’¸é¦é…ç½®
            training_config: è®­ç»ƒé…ç½®
        """
        super(KnowledgeDistillationTrainer, self).__init__()
        
        self.save_hyperparameters()
        
        # åŠ è½½Teacheræ¨¡å‹
        print(f"Loading teacher model from {teacher_model_path}")
        self.teacher_model = self._load_teacher_model(teacher_model_path)
        self.teacher_model.eval()
        
        # å†»ç»“Teacheræ¨¡å‹å‚æ•°
        for param in self.teacher_model.parameters():
            param.requires_grad = False
        
        # åˆ›å»ºStudentæ¨¡å‹
        print("Creating student model...")
        self.student_model = MobileInpaintingNetwork(
            input_channels=student_config.get('input_channels', 7),
            output_channels=student_config.get('output_channels', 3)
        )
        
        # è’¸é¦é…ç½®
        self.temperature = distillation_config.get('temperature', 4.0)
        self.alpha = distillation_config.get('alpha', 0.7)  # è’¸é¦æŸå¤±æƒé‡
        self.beta = distillation_config.get('beta', 0.3)    # ç¡¬æ ‡ç­¾æŸå¤±æƒé‡
        
        # æ¸©åº¦è°ƒåº¦
        self.temperature_schedule = distillation_config.get('temperature_schedule', {})
        self.current_temperature = self.temperature
        
        # æŸå¤±å‡½æ•°
        self.hard_loss = nn.L1Loss()
        self.kl_div_loss = nn.KLDivLoss(reduction='batchmean')
        self.feature_loss = nn.MSELoss()
        
        # è®­ç»ƒé…ç½®
        self.learning_rate = training_config.get('learning_rate', 1e-4)
        self.weight_decay = training_config.get('weight_decay', 1e-5)
        
        # æ€§èƒ½ç›‘æ§
        self.teacher_performance = {}
        self.student_performance = {}
        self.distillation_metrics = {}
        
        print(f"=== Knowledge Distillation Trainer ===")
        print(f"Teacher parameters: {sum(p.numel() for p in self.teacher_model.parameters()):,}")
        print(f"Student parameters: {sum(p.numel() for p in self.student_model.parameters()):,}")
        print(f"Initial temperature: {self.temperature}")
        print(f"Alpha (distillation weight): {self.alpha}")
        print(f"Beta (hard loss weight): {self.beta}")
    
    def _load_teacher_model(self, model_path: str) -> nn.Module:
        """
        åŠ è½½Teacheræ¨¡å‹
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
        
        Returns:
            teacher_model: Teacheræ¨¡å‹
        """
        if model_path.endswith('.ckpt'):
            # ä»Lightningæ£€æŸ¥ç‚¹åŠ è½½
            teacher_trainer = FrameInterpolationTrainer.load_from_checkpoint(model_path)
            return teacher_trainer.student_model
        else:
            # ä»PyTorchæ£€æŸ¥ç‚¹åŠ è½½
            teacher_model = MobileInpaintingNetwork()
            checkpoint = torch.load(model_path, map_location='cpu')
            
            if 'state_dict' in checkpoint:
                teacher_model.load_state_dict(checkpoint['state_dict'])
            else:
                teacher_model.load_state_dict(checkpoint)
            
            return teacher_model
    
    def _update_temperature(self, epoch: int):
        """
        æ›´æ–°è’¸é¦æ¸©åº¦
        
        Args:
            epoch: å½“å‰epoch
        """
        if 'type' in self.temperature_schedule:
            schedule_type = self.temperature_schedule['type']
            
            if schedule_type == 'linear':
                # çº¿æ€§è¡°å‡
                start_temp = self.temperature_schedule.get('start', self.temperature)
                end_temp = self.temperature_schedule.get('end', 1.0)
                total_epochs = self.temperature_schedule.get('epochs', 100)
                
                progress = min(epoch / total_epochs, 1.0)
                self.current_temperature = start_temp + (end_temp - start_temp) * progress
                
            elif schedule_type == 'cosine':
                # ä½™å¼¦è¡°å‡
                start_temp = self.temperature_schedule.get('start', self.temperature)
                end_temp = self.temperature_schedule.get('end', 1.0)
                total_epochs = self.temperature_schedule.get('epochs', 100)
                
                progress = min(epoch / total_epochs, 1.0)
                self.current_temperature = end_temp + (start_temp - end_temp) * \
                                         (1 + np.cos(np.pi * progress)) / 2
            
            self.log('distillation/temperature', self.current_temperature)
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        return self.student_model(x)
    
    def training_step(self, batch, batch_idx):
        """è®­ç»ƒæ­¥éª¤"""
        input_data, target, input_prev, target_prev = batch
        
        # Studentæ¨¡å‹é¢„æµ‹
        student_output = self.student_model(input_data)
        
        # Teacheræ¨¡å‹é¢„æµ‹ï¼ˆæ— æ¢¯åº¦ï¼‰
        with torch.no_grad():
            teacher_output = self.teacher_model(input_data)
        
        # è®¡ç®—æŸå¤±
        losses = self._compute_distillation_losses(
            student_output, teacher_output, target
        )
        
        total_loss = losses['total']
        
        # è®°å½•æŸå¤±
        for key, value in losses.items():
            self.log(f'train/{key}_loss', value, prog_bar=(key=='total'))
        
        # è®°å½•æ€§èƒ½æŒ‡æ ‡
        with torch.no_grad():
            student_ssim = self._calculate_ssim(student_output, target)
            teacher_ssim = self._calculate_ssim(teacher_output, target)
            
            self.log('train/student_ssim', student_ssim)
            self.log('train/teacher_ssim', teacher_ssim)
            self.log('train/performance_ratio', student_ssim / (teacher_ssim + 1e-8))
        
        return total_loss
    
    def validation_step(self, batch, batch_idx):
        """éªŒè¯æ­¥éª¤"""
        input_data, target, input_prev, target_prev = batch
        
        # æ¨¡å‹é¢„æµ‹
        student_output = self.student_model(input_data)
        
        with torch.no_grad():
            teacher_output = self.teacher_model(input_data)
        
        # è®¡ç®—æŸå¤±
        losses = self._compute_distillation_losses(
            student_output, teacher_output, target
        )
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        student_ssim = self._calculate_ssim(student_output, target)
        teacher_ssim = self._calculate_ssim(teacher_output, target)
        student_psnr = self._calculate_psnr(student_output, target)
        teacher_psnr = self._calculate_psnr(teacher_output, target)
        
        # è®°å½•éªŒè¯æŒ‡æ ‡
        self.log('val/total_loss', losses['total'], prog_bar=True)
        self.log('val/student_ssim', student_ssim, prog_bar=True)
        self.log('val/teacher_ssim', teacher_ssim)
        self.log('val/student_psnr', student_psnr)
        self.log('val/teacher_psnr', teacher_psnr)
        self.log('val/ssim_retention', student_ssim / (teacher_ssim + 1e-8), prog_bar=True)
        self.log('val/psnr_retention', student_psnr / (teacher_psnr + 1e-8))
        
        return {
            'val_loss': losses['total'],
            'student_ssim': student_ssim,
            'teacher_ssim': teacher_ssim,
            'retention_rate': student_ssim / (teacher_ssim + 1e-8)
        }
    
    def _compute_distillation_losses(self, 
                                   student_output: torch.Tensor,
                                   teacher_output: torch.Tensor,
                                   target: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—è’¸é¦æŸå¤±
        
        Args:
            student_output: Studentè¾“å‡º
            teacher_output: Teacherè¾“å‡º
            target: çœŸå€¼ç›®æ ‡
        
        Returns:
            losses: æŸå¤±å­—å…¸
        """
        losses = {}
        
        # ç¡¬æ ‡ç­¾æŸå¤±ï¼ˆä¸çœŸå€¼çš„L1æŸå¤±ï¼‰
        hard_loss = self.hard_loss(student_output, target)
        losses['hard'] = hard_loss
        
        # è½¯æ ‡ç­¾è’¸é¦æŸå¤±ï¼ˆKLæ•£åº¦ï¼‰
        student_soft = F.log_softmax(student_output / self.current_temperature, dim=1)
        teacher_soft = F.softmax(teacher_output / self.current_temperature, dim=1)
        
        kl_loss = self.kl_div_loss(student_soft, teacher_soft) * (self.current_temperature ** 2)
        losses['kl_divergence'] = kl_loss
        
        # ç‰¹å¾è’¸é¦æŸå¤±ï¼ˆè¾“å‡ºç‰¹å¾çš„MSEï¼‰
        feature_loss = self.feature_loss(student_output, teacher_output.detach())
        losses['feature'] = feature_loss
        
        # æ€»æŸå¤±
        total_loss = (self.beta * hard_loss + 
                     self.alpha * kl_loss + 
                     0.1 * feature_loss)
        losses['total'] = total_loss
        
        return losses
    
    def on_train_epoch_start(self):
        """è®­ç»ƒepochå¼€å§‹æ—¶çš„å›è°ƒ"""
        # æ›´æ–°æ¸©åº¦è°ƒåº¦
        self._update_temperature(self.current_epoch)
    
    def configure_optimizers(self):
        """é…ç½®ä¼˜åŒ–å™¨"""
        optimizer = torch.optim.AdamW(
            self.student_model.parameters(),
            lr=self.learning_rate,
            weight_decay=self.weight_decay
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=self.trainer.max_epochs, eta_min=1e-6
        )
        
        return {
            'optimizer': optimizer,
            'lr_scheduler': {
                'scheduler': scheduler,
                'monitor': 'val/total_loss'
            }
        }
    
    def _calculate_ssim(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—SSIMæŒ‡æ ‡"""
        # ç®€åŒ–çš„SSIMè®¡ç®—
        mu1 = pred.mean()
        mu2 = target.mean() 
        sigma1_sq = ((pred - mu1) ** 2).mean()
        sigma2_sq = ((target - mu2) ** 2).mean()
        sigma12 = ((pred - mu1) * (target - mu2)).mean()
        
        c1 = 0.01 ** 2
        c2 = 0.03 ** 2
        
        ssim = ((2 * mu1 * mu2 + c1) * (2 * sigma12 + c2)) / \
               ((mu1 ** 2 + mu2 ** 2 + c1) * (sigma1_sq + sigma2_sq + c2))
        
        return ssim
    
    def _calculate_psnr(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—PSNRæŒ‡æ ‡"""
        mse = F.mse_loss(pred, target)
        psnr = 20 * torch.log10(2.0 / torch.sqrt(mse))  # è¾“å…¥èŒƒå›´[-1,1]
        return psnr


def create_distillation_trainer(teacher_model_path: str,
                               config: Dict[str, Any]) -> KnowledgeDistillationTrainer:
    """
    åˆ›å»ºçŸ¥è¯†è’¸é¦è®­ç»ƒå™¨
    
    Args:
        teacher_model_path: Teacheræ¨¡å‹è·¯å¾„
        config: è®­ç»ƒé…ç½®
    
    Returns:
        trainer: è’¸é¦è®­ç»ƒå™¨
    """
    return KnowledgeDistillationTrainer(
        teacher_model_path=teacher_model_path,
        student_config=config['model'],
        distillation_config=config['distillation'],
        training_config=config['training']
    )


def main():
    """çŸ¥è¯†è’¸é¦è®­ç»ƒä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Knowledge Distillation Training')
    parser.add_argument('--config', type=str, required=True,
                       help='Path to distillation config file')
    parser.add_argument('--teacher', type=str, required=True,
                       help='Path to teacher model')
    parser.add_argument('--output-dir', type=str, default='./distillation_output',
                       help='Output directory')
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    config = load_config(args.config)
    
    # è®¾ç½®éšæœºç§å­
    set_random_seeds(config.get('seed', 42))
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("="*60)
    print("ğŸ“ Knowledge Distillation Training Started")
    print("="*60)
    print(f"Teacher Model: {args.teacher}")
    print(f"Config: {args.config}")
    print(f"Output Dir: {output_dir}")
    print("="*60)
    
    try:
        # åˆ›å»ºæ•°æ®é›†
        print("\nğŸ“Š Setting up datasets...")
        dataloaders = create_multi_game_dataset(
            data_configs=config['datasets'],
            batch_size=config['training']['batch_size'],
            num_workers=config['training']['num_workers']
        )
        
        # åˆ›å»ºè’¸é¦è®­ç»ƒå™¨
        print("\nğŸ“ Creating distillation trainer...")
        distillation_trainer = create_distillation_trainer(
            teacher_model_path=args.teacher,
            config=config
        )
        
        # è®¾ç½®Loggerå’ŒCallbacks
        logger = TensorBoardLogger(
            save_dir=str(output_dir / 'logs'),
            name='knowledge_distillation'
        )
        
        callbacks = [
            ModelCheckpoint(
                dirpath=str(output_dir / 'checkpoints'),
                filename='student-{epoch:02d}-{val/ssim_retention:.4f}',
                monitor='val/ssim_retention',
                mode='max',
                save_top_k=3,
                save_last=True
            ),
            EarlyStopping(
                monitor='val/total_loss',
                patience=20,
                mode='min'
            )
        ]
        
        # åˆ›å»ºLightningè®­ç»ƒå™¨
        trainer = pl.Trainer(
            max_epochs=config['trainer']['max_epochs'],
            accelerator='gpu',
            devices=1,
            precision=16,
            logger=logger,
            callbacks=callbacks,
            gradient_clip_val=1.0,
            deterministic=True
        )
        
        # å¼€å§‹è’¸é¦è®­ç»ƒ
        print("\nğŸ‹ï¸ Starting knowledge distillation...")
        start_time = time.time()
        
        trainer.fit(
            model=distillation_trainer,
            train_dataloaders=dataloaders['train'],
            val_dataloaders=dataloaders['val']
        )
        
        training_time = time.time() - start_time
        print(f"\nâœ… Distillation completed in {training_time/3600:.2f} hours")
        
        # ä¿å­˜æœ€ç»ˆçš„Studentæ¨¡å‹
        student_model_path = output_dir / 'student_model.pth'
        torch.save(distillation_trainer.student_model.state_dict(), student_model_path)
        print(f"Student model saved to {student_model_path}")
        
        # æ€§èƒ½å¯¹æ¯”æµ‹è¯•
        print("\nğŸ“Š Running performance comparison...")
        test_results = trainer.test(
            model=distillation_trainer,
            dataloaders=dataloaders['val']
        )
        
        print(f"Final Test Results: {test_results}")
        print("\nğŸ‰ Knowledge distillation completed successfully!")
        
    except Exception as e:
        print(f"\nâŒ Distillation failed with error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()