"""
@file dataset.py
@brief NoiseBaseé¢„å¤„ç†æ•°æ®é›†

æ ¸å¿ƒåŠŸèƒ½ï¼š
- åŠ è½½é¢„å¤„ç†åçš„NoiseBase 6é€šé“è®­ç»ƒæ•°æ®
- æ”¯æŒæ•°æ®å¢å¼ºå’Œæ‰¹å¤„ç†
- æä¾›PyTorch Datasetæ¥å£

æ•°æ®æ ¼å¼ï¼š
- è¾“å…¥: RGB(3) + Mask(1) + ResidualMV(2) = 6é€šé“
- è¾“å‡º: RGB(3) Ground Truth
- æ”¯æŒå¤šç§patchå¤§å°

@author AIç®—æ³•å›¢é˜Ÿ
@date 2025-08-02
@version 2.0 (ç®€åŒ–ç‰ˆ)
"""

import torch
from torch.utils.data import Dataset, DataLoader
import cv2
import numpy as np
import os
import random
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union
import json
import pickle
from PIL import Image
import torchvision.transforms as transforms
import torch.nn.functional as F


class NoiseBaseDataset(Dataset):
    """
    NoiseBaseé¢„å¤„ç†æ•°æ®é›†
    
    åŠŸèƒ½ï¼š
    - åŠ è½½é¢„å¤„ç†åçš„6é€šé“è®­ç»ƒæ•°æ®
    - æ”¯æŒæ•°æ®å¢å¼ºå’Œpatchæå–
    - æä¾›è®­ç»ƒ/éªŒè¯/æµ‹è¯•åˆ†å‰²
    """
    
    def __init__(self, 
                 data_root: str,
                 scene_name: str,
                 split: str = 'train',
                 patch_size: int = 64,
                 augmentation: bool = True):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        Args:
            data_root: é¢„å¤„ç†åçš„æ•°æ®æ ¹ç›®å½•
            scene_name: åœºæ™¯åç§° (å¦‚ 'bistro1')
            split: æ•°æ®åˆ†å‰² ('train', 'val', 'test')
            patch_size: patchå¤§å°
            augmentation: æ˜¯å¦ä½¿ç”¨æ•°æ®å¢å¼º
        """
        super(NoiseBaseDataset, self).__init__()
        
        self.data_root = Path(data_root)
        self.scene_name = scene_name
        self.split = split
        self.patch_size = patch_size
        self.augmentation = augmentation and (split == 'train')
        
        # åŠ è½½æ•°æ®æ–‡ä»¶åˆ—è¡¨
        self.data_files = self._load_data_files()
        
        # æ•°æ®å¢å¼ºå˜æ¢
        if self.augmentation:
            self.transform = transforms.Compose([
                transforms.RandomHorizontalFlip(p=0.5),
                transforms.RandomVerticalFlip(p=0.3),
            ])
        else:
            self.transform = None
        
        print(f"=== NoiseBaseDataset ({split}) ===")
        print(f"Scene: {scene_name}")
        print(f"Data samples: {len(self.data_files)}")
        print(f"Patch size: {patch_size}x{patch_size}")
        print(f"Augmentation: {self.augmentation}")
    
    def _load_data_files(self) -> List[Path]:
        """åŠ è½½æ•°æ®æ–‡ä»¶åˆ—è¡¨"""
        training_data_dir = self.data_root / self.scene_name / 'training_data'
        
        if not training_data_dir.exists():
            raise FileNotFoundError(f"è®­ç»ƒæ•°æ®ç›®å½•ä¸å­˜åœ¨: {training_data_dir}")
        
        # è·å–æ‰€æœ‰.npyæ–‡ä»¶
        all_files = list(training_data_dir.glob("*.npy"))
        all_files.sort()  # ç¡®ä¿é¡ºåºä¸€è‡´
        
        if len(all_files) == 0:
            raise ValueError(f"æ²¡æœ‰æ‰¾åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶: {training_data_dir}")
        
        # ç®€å•çš„æ•°æ®åˆ†å‰²
        total_files = len(all_files)
        
        if self.split == 'train':
            # å‰80%ä½œä¸ºè®­ç»ƒ
            end_idx = int(total_files * 0.8)
            selected_files = all_files[:end_idx]
        elif self.split == 'val':
            # 80%-95%ä½œä¸ºéªŒè¯
            start_idx = int(total_files * 0.8)
            end_idx = int(total_files * 0.95)
            selected_files = all_files[start_idx:end_idx]
        else:  # test
            # æœ€å5%ä½œä¸ºæµ‹è¯•
            start_idx = int(total_files * 0.95)
            selected_files = all_files[start_idx:]
        
        return selected_files
    
    def __len__(self) -> int:
        """æ•°æ®é›†å¤§å°"""
        return len(self.data_files)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """
        è·å–æ•°æ®é¡¹
        
        Args:
            idx: æ ·æœ¬ç´¢å¼•
            
        Returns:
            sample: åŒ…å«inputå’Œtargetçš„å­—å…¸
        """
        # åŠ è½½æ•°æ®
        data_file = self.data_files[idx]
        sample_data = np.load(data_file)  # [6, H, W]
        
        # åˆ†ç¦»è¾“å…¥å’Œç›®æ ‡
        input_data = sample_data  # 6é€šé“è¾“å…¥: RGB + Mask + ResidualMV
        target_data = sample_data[:3]  # 3é€šé“ç›®æ ‡: RGB (Ground Truth)
        
        # è½¬æ¢ä¸ºtensor
        input_tensor = torch.from_numpy(input_data).float()
        target_tensor = torch.from_numpy(target_data).float()
        
        # éšæœºè£å‰ªåˆ°æŒ‡å®špatchå¤§å°
        if input_tensor.shape[1] > self.patch_size or input_tensor.shape[2] > self.patch_size:
            input_tensor, target_tensor = self._random_crop(input_tensor, target_tensor)
        
        # æ•°æ®å¢å¼º
        if self.transform is not None:
            # å°†è¾“å…¥å’Œç›®æ ‡æ‹¼æ¥è¿›è¡ŒåŒæ­¥å˜æ¢
            combined = torch.cat([input_tensor, target_tensor], dim=0)  # [9, H, W]
            combined = self.transform(combined)
            
            # åˆ†ç¦»å›è¾“å…¥å’Œç›®æ ‡
            input_tensor = combined[:6]
            target_tensor = combined[6:9]
        
        # æ•°æ®å½’ä¸€åŒ–
        input_tensor = self._normalize_input(input_tensor)
        target_tensor = self._normalize_target(target_tensor)
        
        return {
            'input': input_tensor,      # [6, H, W]
            'target': target_tensor,    # [3, H, W]
            'filename': data_file.stem,
            'scene': self.scene_name
        }
    
    def _random_crop(self, input_tensor: torch.Tensor, target_tensor: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        éšæœºè£å‰ª
        
        Args:
            input_tensor: è¾“å…¥å¼ é‡ [6, H, W]
            target_tensor: ç›®æ ‡å¼ é‡ [3, H, W]
            
        Returns:
            cropped_input, cropped_target: è£å‰ªåçš„å¼ é‡
        """
        _, H, W = input_tensor.shape
        
        if H <= self.patch_size and W <= self.patch_size:
            return input_tensor, target_tensor
        
        # éšæœºé€‰æ‹©è£å‰ªä½ç½®
        top = random.randint(0, max(0, H - self.patch_size))
        left = random.randint(0, max(0, W - self.patch_size))
        
        # è£å‰ª
        cropped_input = input_tensor[:, top:top+self.patch_size, left:left+self.patch_size]
        cropped_target = target_tensor[:, top:top+self.patch_size, left:left+self.patch_size]
        
        return cropped_input, cropped_target
    
    def _normalize_input(self, input_tensor: torch.Tensor) -> torch.Tensor:
        """
        å½’ä¸€åŒ–è¾“å…¥æ•°æ®
        
        Args:
            input_tensor: è¾“å…¥å¼ é‡ [6, H, W]
            
        Returns:
            normalized_input: å½’ä¸€åŒ–åçš„è¾“å…¥
        """
        normalized = input_tensor.clone()
        
        # RGBé€šé“ (0-2): å·²ç»åœ¨[0,1]èŒƒå›´å†…ï¼Œä¿æŒä¸å˜
        # Maské€šé“ (3): å·²ç»åœ¨[0,1]èŒƒå›´å†…ï¼Œä¿æŒä¸å˜
        # ResidualMVé€šé“ (4-5): å¯èƒ½éœ€è¦ç¼©æ”¾
        
        mv_channels = normalized[4:6]
        mv_magnitude = torch.sqrt(mv_channels[0]**2 + mv_channels[1]**2)
        max_magnitude = torch.max(mv_magnitude)
        
        if max_magnitude > 0:
            # å°†è¿åŠ¨çŸ¢é‡ç¼©æ”¾åˆ°åˆç†èŒƒå›´
            scale_factor = min(1.0, 10.0 / max_magnitude)
            normalized[4:6] *= scale_factor
        
        return normalized
    
    def _normalize_target(self, target_tensor: torch.Tensor) -> torch.Tensor:
        """
        å½’ä¸€åŒ–ç›®æ ‡æ•°æ®
        
        Args:
            target_tensor: ç›®æ ‡å¼ é‡ [3, H, W]
            
        Returns:
            normalized_target: å½’ä¸€åŒ–åçš„ç›®æ ‡
        """
        # RGBç›®æ ‡å·²ç»åœ¨[0,1]èŒƒå›´å†…ï¼Œç›´æ¥è¿”å›
        return target_tensor


def create_noisebase_dataloader(data_root: str, 
                               scene_name: str,
                               split: str = 'train',
                               batch_size: int = 8,
                               patch_size: int = 64,
                               num_workers: int = 4,
                               shuffle: bool = None) -> DataLoader:
    """
    åˆ›å»ºNoiseBaseæ•°æ®åŠ è½½å™¨
    
    Args:
        data_root: æ•°æ®æ ¹ç›®å½•
        scene_name: åœºæ™¯åç§°
        split: æ•°æ®åˆ†å‰²
        batch_size: æ‰¹æ¬¡å¤§å°
        patch_size: patchå¤§å°
        num_workers: å·¥ä½œçº¿ç¨‹æ•°
        shuffle: æ˜¯å¦æ‰“ä¹±æ•°æ®
        
    Returns:
        dataloader: æ•°æ®åŠ è½½å™¨
    """
    if shuffle is None:
        shuffle = (split == 'train')
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = NoiseBaseDataset(
        data_root=data_root,
        scene_name=scene_name,
        split=split,
        patch_size=patch_size,
        augmentation=(split == 'train')
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=(split == 'train')
    )
    
    return dataloader


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='æµ‹è¯•NoiseBaseæ•°æ®é›†')
    parser.add_argument('--data-root', type=str, required=True, help='é¢„å¤„ç†åçš„æ•°æ®æ ¹ç›®å½•')
    parser.add_argument('--scene', type=str, default='bistro1', help='åœºæ™¯åç§°')
    
    args = parser.parse_args()
    
    print("="*60)
    print("ğŸ§ª æµ‹è¯•NoiseBaseæ•°æ®é›†åŠ è½½")
    print("="*60)
    
    try:
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = create_noisebase_dataloader(
            data_root=args.data_root,
            scene_name=args.scene,
            split='train',
            batch_size=2,
            patch_size=64
        )
        
        print(f"âœ… è®­ç»ƒæ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
        print(f"   æ•°æ®é›†å¤§å°: {len(train_loader.dataset)}")
        print(f"   æ‰¹æ¬¡æ•°: {len(train_loader)}")
        
        # æµ‹è¯•æ•°æ®åŠ è½½
        for i, batch in enumerate(train_loader):
            print(f"\nğŸ“Š æ‰¹æ¬¡ {i+1}:")
            print(f"   è¾“å…¥å½¢çŠ¶: {batch['input'].shape}")
            print(f"   ç›®æ ‡å½¢çŠ¶: {batch['target'].shape}")
            print(f"   è¾“å…¥æ•°å€¼èŒƒå›´: [{batch['input'].min():.3f}, {batch['input'].max():.3f}]")
            print(f"   ç›®æ ‡æ•°å€¼èŒƒå›´: [{batch['target'].min():.3f}, {batch['target'].max():.3f}]")
            print(f"   æ–‡ä»¶å: {batch['filename']}")
            
            if i >= 2:  # åªæµ‹è¯•å‰3ä¸ªæ‰¹æ¬¡
                break
        
        print("\nâœ… æ•°æ®é›†æµ‹è¯•é€šè¿‡ï¼")
        
    except Exception as e:
        print(f"âŒ æ•°æ®é›†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
