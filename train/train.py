#!/usr/bin/env python3
"""
@file train.py
@brief ç§»åŠ¨ç«¯å®æ—¶å¸§å¤–æ’ä¸ç©ºæ´è¡¥å…¨ç³»ç»Ÿè®­ç»ƒè„šæœ¬

åŠŸèƒ½æè¿°ï¼š
- å®Œæ•´çš„ç«¯åˆ°ç«¯è®­ç»ƒæµç¨‹
- æ”¯æŒå¤šæ¸¸æˆæ•°æ®é›†è”åˆè®­ç»ƒ
- çŸ¥è¯†è’¸é¦å’Œé‡åŒ–æ„ŸçŸ¥è®­ç»ƒ
- åˆ†å¸ƒå¼è®­ç»ƒå’Œæ··åˆç²¾åº¦è®­ç»ƒ
- å®æ—¶ç›‘æ§å’Œæ—¥å¿—è®°å½•

è®­ç»ƒé˜¶æ®µï¼š
1. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
2. æ¨¡å‹åˆå§‹åŒ–å’Œé…ç½®
3. è®­ç»ƒå¾ªç¯æ‰§è¡Œ
4. éªŒè¯å’Œæ€§èƒ½è¯„ä¼°
5. æ¨¡å‹ä¿å­˜å’Œéƒ¨ç½²å‡†å¤‡

æ€§èƒ½ç›‘æ§ï¼š
- å®æ—¶æŸå¤±æ›²çº¿
- SSIM/PSNRæŒ‡æ ‡è·Ÿè¸ª
- GPUå†…å­˜å’Œè®¡ç®—åˆ©ç”¨ç‡
- è®­ç»ƒé€Ÿåº¦å’ŒETAä¼°ç®—

@author AIç®—æ³•å›¢é˜Ÿ
@date 2025-07-28
@version 1.0
"""

import os
import sys
import argparse
import yaml
import time
import random
from pathlib import Path
from typing import Dict, Any, Optional
import numpy as np

import torch
import torch.nn as nn
import torch.multiprocessing as mp
from torch.utils.data import DataLoader, ConcatDataset
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor
from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger
from pytorch_lightning.strategies import DDPStrategy
import wandb

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src', 'npu', 'networks'))

# å¯¼å…¥è®­ç»ƒç»„ä»¶
from training_framework import FrameInterpolationTrainer, create_trainer
from dataset import GameSceneDataModule, create_data_module
from datasets.cod_mobile_dataset import create_cod_mobile_dataset
from datasets.honor_of_kings_dataset import create_honor_of_kings_dataset
from mobile_inpainting_network import create_mobile_inpainting_network

# è®¾ç½®éšæœºç§å­
def set_random_seeds(seed: int = 42):
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    pl.seed_everything(seed)
    print(f"Random seeds set to {seed}")


def load_config(config_path: str) -> Dict[str, Any]:
    """
    åŠ è½½è®­ç»ƒé…ç½®æ–‡ä»¶
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
    
    Returns:
        config: é…ç½®å­—å…¸
    """
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        print(f"Config loaded from {config_path}")
        return config
    except Exception as e:
        print(f"Error loading config: {e}")
        raise


def create_multi_game_dataset(data_configs: Dict[str, Any], 
                             batch_size: int = 32,
                             num_workers: int = 4) -> Dict[str, DataLoader]:
    """
    åˆ›å»ºå¤šæ¸¸æˆè”åˆæ•°æ®é›†
    
    Args:
        data_configs: å¤šæ¸¸æˆæ•°æ®é…ç½®
        batch_size: æ‰¹æ¬¡å¤§å°
        num_workers: å·¥ä½œçº¿ç¨‹æ•°
    
    Returns:
        dataloaders: æ•°æ®åŠ è½½å™¨å­—å…¸
    """
    train_datasets = []
    val_datasets = []
    test_datasets = []
    
    # éå†æ‰€æœ‰æ¸¸æˆé…ç½®
    for game_name, game_config in data_configs.items():
        print(f"\n=== Loading {game_name} Dataset ===")
        
        try:
            # æ ¹æ®æ¸¸æˆç±»å‹åˆ›å»ºå¯¹åº”æ•°æ®é›†
            if game_name == 'cod_mobile':
                train_ds = create_cod_mobile_dataset(
                    data_root=game_config['data_root'],
                    split_file=game_config['train_split'],
                    patch_size=game_config['patch_size'],
                    mode='train'
                )
                val_ds = create_cod_mobile_dataset(
                    data_root=game_config['data_root'],
                    split_file=game_config['val_split'],
                    patch_size=game_config['patch_size'],
                    mode='val'
                )
                
            elif game_name == 'honor_of_kings':
                train_ds = create_honor_of_kings_dataset(
                    data_root=game_config['data_root'],
                    split_file=game_config['train_split'],
                    patch_size=game_config['patch_size'],
                    mode='train'
                )
                val_ds = create_honor_of_kings_dataset(
                    data_root=game_config['data_root'],
                    split_file=game_config['val_split'],
                    patch_size=game_config['patch_size'],
                    mode='val'
                )
            
            # TODO: æ·»åŠ å…¶ä»–æ¸¸æˆæ•°æ®é›†
            # elif game_name == 'qq_speed':
            #     ...
            # elif game_name == 'genshin_impact':
            #     ...
            
            else:
                print(f"Warning: Unknown game type {game_name}, skipping...")
                continue
            
            train_datasets.append(train_ds)
            val_datasets.append(val_ds)
            
            print(f"{game_name} - Train: {len(train_ds)}, Val: {len(val_ds)}")
            
        except Exception as e:
            print(f"Error loading {game_name} dataset: {e}")
            continue
    
    if not train_datasets:
        raise ValueError("No valid datasets loaded!")
    
    # åˆå¹¶æ•°æ®é›†
    combined_train = ConcatDataset(train_datasets)
    combined_val = ConcatDataset(val_datasets)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        combined_train,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=True,
        persistent_workers=True if num_workers > 0 else False
    )
    
    val_loader = DataLoader(
        combined_val,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=False,
        persistent_workers=True if num_workers > 0 else False
    )
    
    print(f"\n=== Combined Dataset Statistics ===")
    print(f"Total Train Samples: {len(combined_train)}")
    print(f"Total Val Samples: {len(combined_val)}")
    print(f"Batch Size: {batch_size}")
    print(f"Train Batches: {len(train_loader)}")
    print(f"Val Batches: {len(val_loader)}")
    
    return {
        'train': train_loader,
        'val': val_loader
    }


def setup_logging(config: Dict[str, Any]) -> List[pl.loggers.Logger]:
    """
    è®¾ç½®æ—¥å¿—è®°å½•å™¨
    
    Args:
        config: è®­ç»ƒé…ç½®
    
    Returns:
        loggers: æ—¥å¿—è®°å½•å™¨åˆ—è¡¨
    """
    loggers = []
    
    # TensorBoardæ—¥å¿—
    if config.get('tensorboard', {}).get('enabled', True):
        tb_logger = TensorBoardLogger(
            save_dir=config['logging']['log_dir'],
            name='mobile_inpainting',
            version=f"v{config['experiment']['version']}",
            default_hp_metric=False
        )
        loggers.append(tb_logger)
        print(f"TensorBoard logging enabled: {tb_logger.log_dir}")
    
    # W&Bæ—¥å¿—
    if config.get('wandb', {}).get('enabled', False):
        wandb_logger = WandbLogger(
            project=config['wandb']['project'],
            name=config['experiment']['name'],
            version=config['experiment']['version'],
            save_dir=config['logging']['log_dir']
        )
        loggers.append(wandb_logger)
        print(f"W&B logging enabled: {config['wandb']['project']}")
    
    return loggers


def setup_callbacks(config: Dict[str, Any]) -> List[pl.Callback]:
    """
    è®¾ç½®è®­ç»ƒå›è°ƒå‡½æ•°
    
    Args:
        config: è®­ç»ƒé…ç½®
    
    Returns:
        callbacks: å›è°ƒå‡½æ•°åˆ—è¡¨
    """
    callbacks = []
    
    # æ¨¡å‹æ£€æŸ¥ç‚¹
    checkpoint_callback = ModelCheckpoint(
        dirpath=config['logging']['checkpoint_dir'],
        filename='mobile_inpainting-{epoch:02d}-{val/ssim:.4f}',
        monitor='val/ssim',
        mode='max',
        save_top_k=3,
        save_last=True,
        verbose=True
    )
    callbacks.append(checkpoint_callback)
    
    # æ—©åœ
    if config.get('early_stopping', {}).get('enabled', True):
        early_stop_callback = EarlyStopping(
            monitor='val/total_loss',
            patience=config['early_stopping']['patience'],
            mode='min',
            verbose=True
        )
        callbacks.append(early_stop_callback)
    
    # å­¦ä¹ ç‡ç›‘æ§
    lr_monitor = LearningRateMonitor(logging_interval='step')
    callbacks.append(lr_monitor)
    
    return callbacks


def create_trainer_instance(config: Dict[str, Any],
                           loggers: List[pl.loggers.Logger],
                           callbacks: List[pl.Callback]) -> pl.Trainer:
    """
    åˆ›å»ºPyTorch Lightningè®­ç»ƒå™¨
    
    Args:
        config: è®­ç»ƒé…ç½®
        loggers: æ—¥å¿—è®°å½•å™¨
        callbacks: å›è°ƒå‡½æ•°
    
    Returns:
        trainer: Lightningè®­ç»ƒå™¨
    """
    trainer_config = config['trainer']
    
    # åˆ†å¸ƒå¼è®­ç»ƒç­–ç•¥
    strategy = None
    if trainer_config.get('distributed', False):
        strategy = DDPStrategy(
            find_unused_parameters=False,
            gradient_as_bucket_view=True
        )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = pl.Trainer(
        max_epochs=trainer_config['max_epochs'],
        accelerator=trainer_config.get('accelerator', 'gpu'),
        devices=trainer_config.get('devices', 1),
        strategy=strategy,
        precision=trainer_config.get('precision', 32),
        gradient_clip_val=trainer_config.get('gradient_clip_val', 1.0),
        accumulate_grad_batches=trainer_config.get('accumulate_grad_batches', 1),
        check_val_every_n_epoch=trainer_config.get('check_val_every_n_epoch', 1),
        log_every_n_steps=trainer_config.get('log_every_n_steps', 50),
        enable_checkpointing=True,
        enable_progress_bar=True,
        enable_model_summary=True,
        logger=loggers,
        callbacks=callbacks,
        deterministic=True
    )
    
    return trainer


def main():
    """è®­ç»ƒä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='Mobile Frame Interpolation Training')
    parser.add_argument('--config', type=str, required=True,
                       help='Path to training config file')
    parser.add_argument('--resume', type=str, default=None,
                       help='Path to checkpoint to resume from')
    parser.add_argument('--test-only', action='store_true',
                       help='Only run testing')
    parser.add_argument('--debug', action='store_true',
                       help='Enable debug mode')
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    config = load_config(args.config)
    
    # è®¾ç½®éšæœºç§å­ 
    set_random_seeds(config.get('seed', 42))
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(config['logging']['log_dir'], exist_ok=True)
    os.makedirs(config['logging']['checkpoint_dir'], exist_ok=True)
    
    print("="*60)
    print("ğŸš€ Mobile Frame Interpolation Training Started")
    print("="*60)
    print(f"Experiment: {config['experiment']['name']}")
    print(f"Version: {config['experiment']['version']}")
    print(f"Config: {args.config}")
    if args.resume:
        print(f"Resume from: {args.resume}")
    print("="*60)
    
    try:
        # åˆ›å»ºæ•°æ®é›†
        print("\nğŸ“Š Setting up datasets...")
        dataloaders = create_multi_game_dataset(
            data_configs=config['datasets'],
            batch_size=config['training']['batch_size'],
            num_workers=config['training']['num_workers']
        )
        
        # åˆ›å»ºæ¨¡å‹
        print("\nğŸ§  Creating model...")
        model_trainer = create_trainer(
            model_config=config['model'],
            training_config=config['training'],
            teacher_model_path=config['training'].get('teacher_model_path')
        )
        
        # è®¾ç½®æ—¥å¿—å’Œå›è°ƒ
        print("\nğŸ“ Setting up logging and callbacks...")
        loggers = setup_logging(config)
        callbacks = setup_callbacks(config)
        
        # åˆ›å»ºè®­ç»ƒå™¨
        print("\nâš¡ Creating PyTorch Lightning trainer...")
        trainer = create_trainer_instance(config, loggers, callbacks)
        
        # å¼€å§‹è®­ç»ƒ
        if not args.test_only:
            print("\nğŸ‹ï¸ Starting training...")
            start_time = time.time()
            
            trainer.fit(
                model=model_trainer,
                train_dataloaders=dataloaders['train'],
                val_dataloaders=dataloaders['val'],
                ckpt_path=args.resume
            )
            
            training_time = time.time() - start_time
            print(f"\nâœ… Training completed in {training_time/3600:.2f} hours")
        
        # è¿è¡Œæµ‹è¯•
        if 'test' in dataloaders or args.test_only:
            print("\nğŸ§ª Running final testing...")
            if args.test_only and args.resume:
                # ä»æ£€æŸ¥ç‚¹åŠ è½½æ¨¡å‹è¿›è¡Œæµ‹è¯•
                model_trainer = model_trainer.load_from_checkpoint(args.resume)
            
            test_results = trainer.test(
                model=model_trainer,
                dataloaders=dataloaders.get('test', dataloaders['val'])
            )
            print(f"Test Results: {test_results}")
        
        print("\nğŸ‰ All tasks completed successfully!")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ Training interrupted by user")
        sys.exit(1)
        
    except Exception as e:
        print(f"\nâŒ Training failed with error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    # è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•
    mp.set_start_method('spawn', force=True)
    
    main()