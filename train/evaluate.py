#!/usr/bin/env python3
"""
@file evaluate.py
@brief æ¨¡å‹è¯„ä¼°è„šæœ¬

åŠŸèƒ½æè¿°ï¼š
- å…¨é¢çš„æ¨¡å‹æ€§èƒ½è¯„ä¼°
- å¤šç»´åº¦æŒ‡æ ‡è®¡ç®—å’Œåˆ†æ
- ä¸åŒæ¸¸æˆåœºæ™¯çš„å¯¹æ¯”æµ‹è¯•
- ç”Ÿæˆè¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š

è¯„ä¼°æŒ‡æ ‡ï¼š
- ç”»è´¨æŒ‡æ ‡ï¼šSSIM, PSNR, LPIPS
- æ€§èƒ½æŒ‡æ ‡ï¼šæ¨ç†å»¶è¿Ÿ, å†…å­˜å ç”¨, FPS
- ç¨³å®šæ€§æŒ‡æ ‡ï¼šæ—¶åºä¸€è‡´æ€§, æŠ–åŠ¨æ£€æµ‹
- é²æ£’æ€§æµ‹è¯•ï¼šè¾¹ç•Œæƒ…å†µå¤„ç†

è¾“å‡ºå†…å®¹ï¼š
- é‡åŒ–è¯„ä¼°ç»“æœ
- å¯è§†åŒ–å¯¹æ¯”å›¾è¡¨
- æ€§èƒ½åˆ†ææŠ¥å‘Š
- æ”¹è¿›å»ºè®®

@author AIç®—æ³•å›¢é˜Ÿ
@date 2025-07-28
@version 1.0
"""

import os
import sys
import argparse
import time
import json
from pathlib import Path
from typing import Dict, List, Tuple, Any
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
import cv2
from skimage.metrics import structural_similarity as ssim
from skimage.metrics import peak_signal_noise_ratio as psnr

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src', 'npu', 'networks'))

from mobile_inpainting_network import MobileInpaintingNetwork
from training_framework import FrameInterpolationTrainer
from train import create_multi_game_dataset, load_config


class ModelEvaluator:
    """
    æ¨¡å‹è¯„ä¼°å™¨
    
    æä¾›å…¨é¢çš„æ¨¡å‹æ€§èƒ½è¯„ä¼°å’Œåˆ†æåŠŸèƒ½
    """
    
    def __init__(self, 
                 model_path: str,
                 config: Dict[str, Any],
                 output_dir: str = './evaluation_results'):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            config: è¯„ä¼°é…ç½®
            output_dir: è¾“å‡ºç›®å½•
        """
        self.model_path = model_path
        self.config = config
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½æ¨¡å‹
        self.model = self._load_model(model_path)
        self.model.eval()
        
        # è®¾ç½®è®¾å¤‡
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        
        # è¯„ä¼°ç»“æœå­˜å‚¨
        self.results = {
            'quality_metrics': {},
            'performance_metrics': {},
            'stability_metrics': {},
            'per_game_results': {}
        }
        
        print(f"=== Model Evaluator ===")
        print(f"Model: {model_path}")
        print(f"Device: {self.device}")
        print(f"Output: {output_dir}")
    
    def _load_model(self, model_path: str) -> torch.nn.Module:
        """
        åŠ è½½æ¨¡å‹
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
        
        Returns:
            model: åŠ è½½çš„æ¨¡å‹
        """
        if model_path.endswith('.ckpt'):
            # Lightningæ£€æŸ¥ç‚¹
            trainer = FrameInterpolationTrainer.load_from_checkpoint(model_path)
            return trainer.student_model
        else:
            # PyTorchæ£€æŸ¥ç‚¹
            model = MobileInpaintingNetwork()
            checkpoint = torch.load(model_path, map_location='cpu')
            
            if 'state_dict' in checkpoint:
                model.load_state_dict(checkpoint['state_dict'])
            else:
                model.load_state_dict(checkpoint)
            
            return model
    
    def evaluate_quality_metrics(self, dataloader: DataLoader) -> Dict[str, float]:
        """
        è¯„ä¼°ç”»è´¨æŒ‡æ ‡
        
        Args:
            dataloader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
        
        Returns:
            metrics: ç”»è´¨æŒ‡æ ‡å­—å…¸
        """
        print("\nğŸ“Š Evaluating quality metrics...")
        
        ssim_scores = []
        psnr_scores = []
        l1_errors = []
        
        with torch.no_grad():
            for batch_idx, (input_data, target, _, _) in enumerate(dataloader):
                input_data = input_data.to(self.device)
                target = target.to(self.device)
                
                # æ¨¡å‹é¢„æµ‹
                pred = self.model(input_data)
                
                # è½¬æ¢åˆ°numpyç”¨äºè®¡ç®—æŒ‡æ ‡
                pred_np = self._tensor_to_numpy(pred)
                target_np = self._tensor_to_numpy(target)
                
                # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æŒ‡æ ‡
                batch_ssim = []
                batch_psnr = []
                batch_l1 = []
                
                for i in range(pred_np.shape[0]):
                    # SSIMè®¡ç®—
                    sample_ssim = ssim(
                        target_np[i].transpose(1, 2, 0),
                        pred_np[i].transpose(1, 2, 0),
                        multichannel=True,
                        data_range=2.0  # [-1, 1]èŒƒå›´
                    )
                    batch_ssim.append(sample_ssim)
                    
                    # PSNRè®¡ç®—
                    sample_psnr = psnr(
                        target_np[i].transpose(1, 2, 0),
                        pred_np[i].transpose(1, 2, 0),
                        data_range=2.0
                    )
                    batch_psnr.append(sample_psnr)
                    
                    # L1è¯¯å·®
                    sample_l1 = np.mean(np.abs(target_np[i] - pred_np[i]))
                    batch_l1.append(sample_l1)
                
                ssim_scores.extend(batch_ssim)
                psnr_scores.extend(batch_psnr)
                l1_errors.extend(batch_l1)
                
                if batch_idx % 50 == 0:
                    print(f"Processed {batch_idx + 1}/{len(dataloader)} batches")
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        metrics = {
            'ssim_mean': np.mean(ssim_scores),
            'ssim_std': np.std(ssim_scores),
            'ssim_min': np.min(ssim_scores),
            'ssim_max': np.max(ssim_scores),
            'psnr_mean': np.mean(psnr_scores),
            'psnr_std': np.std(psnr_scores),
            'psnr_min': np.min(psnr_scores),
            'psnr_max': np.max(psnr_scores),
            'l1_mean': np.mean(l1_errors),
            'l1_std': np.std(l1_errors)
        }
        
        print(f"SSIM: {metrics['ssim_mean']:.4f} Â± {metrics['ssim_std']:.4f}")
        print(f"PSNR: {metrics['psnr_mean']:.2f} Â± {metrics['psnr_std']:.2f} dB")
        print(f"L1 Error: {metrics['l1_mean']:.4f} Â± {metrics['l1_std']:.4f}")
        
        return metrics
    
    def evaluate_performance_metrics(self, dataloader: DataLoader) -> Dict[str, float]:
        """
        è¯„ä¼°æ€§èƒ½æŒ‡æ ‡
        
        Args:
            dataloader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
        
        Returns:
            metrics: æ€§èƒ½æŒ‡æ ‡å­—å…¸
        """
        print("\nâš¡ Evaluating performance metrics...")
        
        # é¢„çƒ­
        warmup_batches = 10
        for i, (input_data, _, _, _) in enumerate(dataloader):
            if i >= warmup_batches:
                break
            input_data = input_data.to(self.device)
            with torch.no_grad():
                _ = self.model(input_data)
        
        # æ€§èƒ½æµ‹è¯•
        inference_times = []
        memory_usage = []
        
        torch.cuda.synchronize() if self.device.type == 'cuda' else None
        
        with torch.no_grad():
            for batch_idx, (input_data, _, _, _) in enumerate(dataloader):
                input_data = input_data.to(self.device)
                
                # è®°å½•å†…å­˜ä½¿ç”¨ï¼ˆGPUï¼‰
                if self.device.type == 'cuda':
                    torch.cuda.reset_peak_memory_stats()
                
                # è®¡æ—¶æ¨ç†
                start_time = time.time()
                
                if self.device.type == 'cuda':
                    torch.cuda.synchronize()
                
                pred = self.model(input_data)
                
                if self.device.type == 'cuda':
                    torch.cuda.synchronize()
                
                end_time = time.time()
                
                # è®°å½•æŒ‡æ ‡
                batch_time = (end_time - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
                inference_times.append(batch_time / input_data.shape[0])  # æ¯æ ·æœ¬æ—¶é—´
                
                if self.device.type == 'cuda':
                    peak_memory = torch.cuda.max_memory_allocated() / 1024 / 1024  # MB
                    memory_usage.append(peak_memory)
                
                if batch_idx >= 100:  # é™åˆ¶æµ‹è¯•æ ·æœ¬æ•°é‡
                    break
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        metrics = {
            'inference_time_mean': np.mean(inference_times),
            'inference_time_std': np.std(inference_times),
            'inference_time_min': np.min(inference_times),
            'inference_time_max': np.max(inference_times),
            'fps_mean': 1000.0 / np.mean(inference_times),
            'memory_usage_mean': np.mean(memory_usage) if memory_usage else 0,
            'memory_usage_max': np.max(memory_usage) if memory_usage else 0
        }
        
        print(f"Inference Time: {metrics['inference_time_mean']:.2f} Â± {metrics['inference_time_std']:.2f} ms")
        print(f"FPS: {metrics['fps_mean']:.1f}")
        if memory_usage:
            print(f"GPU Memory: {metrics['memory_usage_mean']:.1f} MB (peak: {metrics['memory_usage_max']:.1f} MB)")
        
        return metrics
    
    def evaluate_temporal_consistency(self, dataloader: DataLoader) -> Dict[str, float]:
        """
        è¯„ä¼°æ—¶åºä¸€è‡´æ€§
        
        Args:
            dataloader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
        
        Returns:
            metrics: æ—¶åºä¸€è‡´æ€§æŒ‡æ ‡
        """
        print("\nğŸ¬ Evaluating temporal consistency...")
        
        frame_differences = []
        optical_flow_errors = []
        
        with torch.no_grad():
            prev_pred = None
            prev_target = None
            
            for batch_idx, (input_data, target, input_prev, target_prev) in enumerate(dataloader):
                input_data = input_data.to(self.device)
                target = target.to(self.device)
                input_prev = input_prev.to(self.device)
                target_prev = target_prev.to(self.device)
                
                # é¢„æµ‹å½“å‰å¸§å’Œå‰ä¸€å¸§
                pred_current = self.model(input_data)
                pred_previous = self.model(input_prev)
                
                # è®¡ç®—å¸§é—´å·®å¼‚
                pred_diff = torch.abs(pred_current - pred_previous).mean(dim=1)  # [B, H, W]
                target_diff = torch.abs(target - target_prev).mean(dim=1)
                
                # æ—¶åºä¸€è‡´æ€§è¯¯å·®
                temporal_error = F.l1_loss(pred_diff, target_diff)
                frame_differences.append(temporal_error.item())
                
                if batch_idx >= 50:  # é™åˆ¶æµ‹è¯•æ ·æœ¬
                    break
        
        metrics = {
            'temporal_consistency_mean': np.mean(frame_differences),
            'temporal_consistency_std': np.std(frame_differences),
            'temporal_stability_score': 1.0 - np.mean(frame_differences)  # ç¨³å®šæ€§è¯„åˆ†
        }
        
        print(f"Temporal Consistency: {metrics['temporal_consistency_mean']:.4f} Â± {metrics['temporal_consistency_std']:.4f}")
        print(f"Stability Score: {metrics['temporal_stability_score']:.4f}")
        
        return metrics
    
    def generate_visual_samples(self, dataloader: DataLoader, num_samples: int = 10):
        """
        ç”Ÿæˆå¯è§†åŒ–æ ·æœ¬
        
        Args:
            dataloader: æ•°æ®åŠ è½½å™¨
            num_samples: æ ·æœ¬æ•°é‡
        """
        print(f"\nğŸ–¼ï¸ Generating {num_samples} visual samples...")
        
        samples_dir = self.output_dir / 'visual_samples'
        samples_dir.mkdir(exist_ok=True)
        
        with torch.no_grad():
            sample_count = 0
            for batch_idx, (input_data, target, _, _) in enumerate(dataloader):
                input_data = input_data.to(self.device)
                target = target.to(self.device)
                
                pred = self.model(input_data)
                
                # ä¿å­˜æ ·æœ¬
                batch_size = input_data.shape[0]
                for i in range(batch_size):
                    if sample_count >= num_samples:
                        break
                    
                    # è½¬æ¢ä¸ºå¯è§†åŒ–æ ¼å¼
                    input_rgb = self._tensor_to_image(input_data[i, :3])  # RGBé€šé“
                    input_mask = input_data[i, 3].cpu().numpy()  # æ©ç é€šé“
                    target_img = self._tensor_to_image(target[i])
                    pred_img = self._tensor_to_image(pred[i])
                    
                    # åˆ›å»ºå¯¹æ¯”å›¾
                    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
                    
                    axes[0, 0].imshow(input_rgb)
                    axes[0, 0].set_title('Input RGB')
                    axes[0, 0].axis('off')
                    
                    axes[0, 1].imshow(input_mask, cmap='gray')
                    axes[0, 1].set_title('Input Mask')
                    axes[0, 1].axis('off')
                    
                    axes[1, 0].imshow(target_img)
                    axes[1, 0].set_title('Ground Truth')
                    axes[1, 0].axis('off')
                    
                    axes[1, 1].imshow(pred_img)
                    axes[1, 1].set_title('Prediction')
                    axes[1, 1].axis('off')
                    
                    plt.tight_layout()
                    plt.savefig(samples_dir / f'sample_{sample_count:03d}.png', dpi=150, bbox_inches='tight')
                    plt.close()
                    
                    sample_count += 1
                
                if sample_count >= num_samples:
                    break
        
        print(f"Visual samples saved to {samples_dir}")
    
    def run_full_evaluation(self, dataloaders: Dict[str, DataLoader]) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´è¯„ä¼°
        
        Args:
            dataloaders: æ•°æ®åŠ è½½å™¨å­—å…¸
        
        Returns:
            results: å®Œæ•´è¯„ä¼°ç»“æœ
        """
        print("="*60)
        print("ğŸ§ª Starting Full Model Evaluation")
        print("="*60)
        
        # ä½¿ç”¨éªŒè¯é›†è¿›è¡Œè¯„ä¼°
        test_loader = dataloaders.get('test', dataloaders['val'])
        
        # ç”»è´¨æŒ‡æ ‡è¯„ä¼°
        quality_metrics = self.evaluate_quality_metrics(test_loader)
        self.results['quality_metrics'] = quality_metrics
        
        # æ€§èƒ½æŒ‡æ ‡è¯„ä¼°
        performance_metrics = self.evaluate_performance_metrics(test_loader)
        self.results['performance_metrics'] = performance_metrics
        
        # æ—¶åºä¸€è‡´æ€§è¯„ä¼°
        stability_metrics = self.evaluate_temporal_consistency(test_loader)
        self.results['stability_metrics'] = stability_metrics
        
        # ç”Ÿæˆå¯è§†åŒ–æ ·æœ¬
        self.generate_visual_samples(test_loader)
        
        # ä¿å­˜ç»“æœ
        self._save_results()
        
        # ç”ŸæˆæŠ¥å‘Š
        self._generate_report()
        
        return self.results
    
    def _tensor_to_numpy(self, tensor: torch.Tensor) -> np.ndarray:
        """å°†tensorè½¬æ¢ä¸ºnumpyæ•°ç»„"""
        return tensor.detach().cpu().numpy()
    
    def _tensor_to_image(self, tensor: torch.Tensor) -> np.ndarray:
        """å°†tensorè½¬æ¢ä¸ºå¯æ˜¾ç¤ºçš„å›¾åƒ"""
        img = tensor.detach().cpu().numpy()
        img = np.transpose(img, (1, 2, 0))  # CHW -> HWC
        img = (img + 1) / 2  # [-1, 1] -> [0, 1]
        img = np.clip(img, 0, 1)
        return img
    
    def _save_results(self):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        results_file = self.output_dir / 'evaluation_results.json'
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        print(f"\nResults saved to {results_file}")
    
    def _generate_report(self):
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        report_file = self.output_dir / 'evaluation_report.md'
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# Mobile Inpainting Network - Evaluation Report\n\n")
            f.write(f"**Model:** {self.model_path}\n")
            f.write(f"**Evaluation Date:** {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # ç”»è´¨æŒ‡æ ‡
            f.write("## Quality Metrics\n\n")
            quality = self.results['quality_metrics']
            f.write(f"- **SSIM:** {quality['ssim_mean']:.4f} Â± {quality['ssim_std']:.4f}\n")
            f.write(f"- **PSNR:** {quality['psnr_mean']:.2f} Â± {quality['psnr_std']:.2f} dB\n")
            f.write(f"- **L1 Error:** {quality['l1_mean']:.4f} Â± {quality['l1_std']:.4f}\n\n")
            
            # æ€§èƒ½æŒ‡æ ‡
            f.write("## Performance Metrics\n\n")
            perf = self.results['performance_metrics']
            f.write(f"- **Inference Time:** {perf['inference_time_mean']:.2f} Â± {perf['inference_time_std']:.2f} ms\n")
            f.write(f"- **FPS:** {perf['fps_mean']:.1f}\n")
            if 'memory_usage_mean' in perf and perf['memory_usage_mean'] > 0:
                f.write(f"- **GPU Memory:** {perf['memory_usage_mean']:.1f} MB (peak: {perf['memory_usage_max']:.1f} MB)\n")
            f.write("\n")
            
            # ç¨³å®šæ€§æŒ‡æ ‡
            f.write("## Stability Metrics\n\n")
            stability = self.results['stability_metrics']
            f.write(f"- **Temporal Consistency:** {stability['temporal_consistency_mean']:.4f} Â± {stability['temporal_consistency_std']:.4f}\n")
            f.write(f"- **Stability Score:** {stability['temporal_stability_score']:.4f}\n\n")
            
            # æ€»ç»“
            f.write("## Summary\n\n")
            if quality['ssim_mean'] > 0.90:
                f.write("âœ… Excellent image quality (SSIM > 0.90)\n")
            elif quality['ssim_mean'] > 0.85:
                f.write("âœ… Good image quality (SSIM > 0.85)\n")
            else:
                f.write("âš ï¸ Image quality needs improvement (SSIM < 0.85)\n")
            
            if perf['inference_time_mean'] < 5.0:
                f.write("âœ… Excellent inference speed (< 5ms)\n")
            elif perf['inference_time_mean'] < 10.0:
                f.write("âœ… Good inference speed (< 10ms)\n")
            else:
                f.write("âš ï¸ Inference speed needs optimization (> 10ms)\n")
        
        print(f"Report saved to {report_file}")


def main():
    """è¯„ä¼°ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Model Evaluation')
    parser.add_argument('--model', type=str, required=True,
                       help='Path to model checkpoint')
    parser.add_argument('--config', type=str, required=True,
                       help='Path to evaluation config')
    parser.add_argument('--output-dir', type=str, default='./evaluation_results',
                       help='Output directory')
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    config = load_config(args.config)
    
    print("="*60)
    print("ğŸ§ª Model Evaluation Started")
    print("="*60)
    print(f"Model: {args.model}")
    print(f"Config: {args.config}")
    print(f"Output: {args.output_dir}")
    print("="*60)
    
    try:
        # åˆ›å»ºæ•°æ®é›†
        print("\nğŸ“Š Loading test datasets...")
        dataloaders = create_multi_game_dataset(
            data_configs=config['datasets'],
            batch_size=config.get('batch_size', 16),
            num_workers=config.get('num_workers', 4)
        )
        
        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = ModelEvaluator(
            model_path=args.model,
            config=config,
            output_dir=args.output_dir
        )
        
        # è¿è¡Œè¯„ä¼°
        results = evaluator.run_full_evaluation(dataloaders)
        
        print("\nğŸ‰ Evaluation completed successfully!")
        print(f"Results saved to {args.output_dir}")
        
    except Exception as e:
        print(f"\nâŒ Evaluation failed with error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()