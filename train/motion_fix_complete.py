#!/usr/bin/env python3
"""
NoiseBaseè¿åŠ¨çŸ¢é‡ä¿®å¤å®Œæ•´æ–¹æ¡ˆ
å®ç°æ­£ç¡®çš„3Dåˆ°2DæŠ•å½±è®¡ç®—å’Œå¤–æ¨ç³»æ•°æ”¯æŒ

åŸºäºmotionè®¡ç®—ä¿®æ­£.mdä¸­çš„å¯¹è¯å†…å®¹å®ç°
"""

import numpy as np
from typing import Dict, Tuple, Optional
from pathlib import Path

class MotionVectorFixer:
    """è¿åŠ¨çŸ¢é‡ä¿®å¤å™¨ - å®ç°æ­£ç¡®çš„3Dåˆ°2DæŠ•å½±"""
    
    def __init__(self, debug: bool = True):
        self.debug = debug
    
    @staticmethod
    def screen_space_projection(world_pos: np.ndarray, 
                               view_proj_matrix: np.ndarray, 
                               height: int, 
                               width: int) -> np.ndarray:
        """
        å°†ä¸–ç•Œç©ºé—´ä½ç½® (3, H, W) æŠ•å½±åˆ°å±å¹•ç©ºé—´åƒç´ åæ ‡ (2, H, W)ã€‚
        
        Args:
            world_pos: ä¸–ç•Œç©ºé—´åæ ‡ï¼Œå½¢çŠ¶ (3, H, W)
            view_proj_matrix: è§†å›¾æŠ•å½±çŸ©é˜µï¼Œå½¢çŠ¶ (4, 4)
            height: å›¾åƒé«˜åº¦
            width: å›¾åƒå®½åº¦
            
        Returns:
            å±å¹•ç©ºé—´åæ ‡ (U, V)ï¼Œå½¢çŠ¶ (2, H, W)
        """
        C, H, W = world_pos.shape
        if C != 3:
            raise ValueError(f"world_pos å¿…é¡»æ˜¯3é€šé“ï¼Œå®é™…ä¸º {C}")

        # 1. æ‰©å±•ä¸ºé½æ¬¡åæ ‡ (4, H, W)
        ones = np.ones((1, H, W), dtype=world_pos.dtype)
        homogeneous_pos = np.concatenate([world_pos, ones], axis=0)

        # 2. çŸ©é˜µæŠ•å½± (view_proj @ pos) -> (4, H, W)
        # ä½¿ç”¨einsumè¿›è¡Œé«˜æ•ˆçš„æ‰¹é‡çŸ©é˜µå‘é‡ä¹˜æ³•
        projected_pos = np.einsum('ij,jhw->ihw', view_proj_matrix, homogeneous_pos)
        
        # 3. é€è§†é™¤æ³• (w-divide)
        w = projected_pos[3:4, :, :]
        # é¿å…é™¤ä»¥é›¶
        epsilon = 1e-5
        w = np.where(w < epsilon, epsilon, w)
        
        # åªå¯¹x, yè¿›è¡Œé€è§†é™¤æ³•
        ndc_pos = projected_pos[:2, :, :] / w

        # 4. NDCåæ ‡[-1, 1]è½¬æ¢ä¸ºå±å¹•åƒç´ åæ ‡[0, W]å’Œ[0, H]
        # U = (ndc_x * 0.5 + 0.5) * W
        # V = (-ndc_y * 0.5 + 0.5) * H  (Yè½´åœ¨NDCå’Œå›¾åƒåæ ‡ç³»ä¸­æ–¹å‘ç›¸å)
        screen_pos_x = (ndc_pos[0] * 0.5 + 0.5) * width
        screen_pos_y = (-ndc_pos[1] * 0.5 + 0.5) * height

        return np.stack([screen_pos_x, screen_pos_y], axis=0)
    
    def compute_screen_space_mv(self, 
                               curr_frame: Dict, 
                               prev_frame: Dict) -> np.ndarray:
        """
        ã€æ ¸å¿ƒä¿®å¤ã€‘æ ¹æ®å‰åä¸¤å¸§çš„3Dä¿¡æ¯è®¡ç®—å±å¹•ç©ºé—´è¿åŠ¨çŸ¢é‡ã€‚
        è¿™ä¸ªMVæè¿°äº†å½“å‰å¸§çš„åƒç´ æ¥è‡ªäºå‰ä¸€å¸§çš„å“ªä¸ªä½ç½®ã€‚
        
        Args:
            curr_frame: å½“å‰å¸§çš„æ•°æ®å­—å…¸
            prev_frame: å‰ä¸€å¸§çš„æ•°æ®å­—å…¸
            
        Returns:
            å±å¹•ç©ºé—´è¿åŠ¨çŸ¢é‡ï¼Œå½¢çŠ¶ (2, H, W)ï¼Œå•ä½ä¸ºåƒç´ ã€‚
        """
        # 1. è·å–å¿…è¦æ•°æ®
        pos_t = curr_frame['position']      # å½“å‰å¸§ä¸–ç•Œåæ ‡ (3, H, W)
        motion_t = curr_frame['motion'][:3] # ä¸–ç•Œç©ºé—´è¿åŠ¨çŸ¢é‡ (3, H, W) (ç¡®ä¿ä¸º3é€šé“)
        
        # è·å–ç›¸æœºçŸ©é˜µ
        if 'camera_params' in curr_frame and 'view_proj_mat' in curr_frame['camera_params']:
            vp_mat_t = curr_frame['camera_params']['view_proj_mat']
        else:
            print("   âš ï¸ ä½¿ç”¨é»˜è®¤æŠ•å½±çŸ©é˜µ")
            vp_mat_t = self._create_default_projection_matrix()
            
        if 'camera_params' in prev_frame and 'view_proj_mat' in prev_frame['camera_params']:
            vp_mat_prev = prev_frame['camera_params']['view_proj_mat']
        else:
            print("   âš ï¸ å‰ä¸€å¸§ä½¿ç”¨é»˜è®¤æŠ•å½±çŸ©é˜µ")
            vp_mat_prev = self._create_default_projection_matrix()

        H, W = pos_t.shape[1], pos_t.shape[2]

        # 2. è®¡ç®—ä¸Šä¸€å¸§çš„ä¸–ç•Œåæ ‡
        # motion å®šä¹‰ä¸º pos_t - pos_{t-1}ï¼Œæ‰€ä»¥ pos_{t-1} = pos_t - motion_t
        pos_prev = pos_t - motion_t

        # 3. å°†å½“å‰å¸§ä¸–ç•Œåæ ‡æŠ•å½±åˆ°å½“å‰å¸§å±å¹•
        screen_pos_t = self.screen_space_projection(pos_t, vp_mat_t, H, W)

        # 4. å°†ä¸Šä¸€å¸§ä¸–ç•Œåæ ‡æŠ•å½±åˆ°ä¸Šä¸€å¸§å±å¹•
        screen_pos_prev = self.screen_space_projection(pos_prev, vp_mat_prev, H, W)
        
        # 5. è®¡ç®—å±å¹•ç©ºé—´è¿åŠ¨çŸ¢é‡ (MV)
        # MV = æ¥æºä½ç½® - ç›®æ ‡ä½ç½® = screen_pos_prev - screen_pos_t
        screen_space_mv = screen_pos_prev - screen_pos_t
        
        # è°ƒè¯•ä¿¡æ¯
        if self.debug:
            mv_magnitude = np.sqrt(screen_space_mv[0]**2 + screen_space_mv[1]**2)
            print(f"   âœ… å·²è®¡ç®—å±å¹•ç©ºé—´MV: å½¢çŠ¶={screen_space_mv.shape}")
            print(f"      åƒç´ è¿åŠ¨ç»Ÿè®¡: å¹³å‡={mv_magnitude.mean():.2f}px, æœ€å¤§={mv_magnitude.max():.2f}px, ä¸­ä½æ•°={np.median(mv_magnitude):.2f}px")
            print(f"      éé›¶è¿åŠ¨åƒç´ æ¯”ä¾‹: {np.mean(mv_magnitude > 0.1):.3f}")

        return screen_space_mv.astype(np.float32)
    
    def _create_default_projection_matrix(self) -> np.ndarray:
        """åˆ›å»ºé»˜è®¤é€è§†æŠ•å½±çŸ©é˜µ"""
        # åˆ›å»ºä¸€ä¸ªåŸºæœ¬çš„é€è§†æŠ•å½±çŸ©é˜µ
        # FOV = 60åº¦, aspect = 16/9, near = 0.1, far = 100
        fov = np.pi / 3  # 60åº¦
        aspect = 16.0 / 9.0
        near, far = 0.1, 100.0
        
        f = 1.0 / np.tan(fov / 2)
        
        projection = np.array([
            [f / aspect, 0, 0, 0],
            [0, f, 0, 0], 
            [0, 0, (far + near) / (near - far), (2 * far * near) / (near - far)],
            [0, 0, -1, 0]
        ], dtype=np.float32)
        
        return projection
    
    def generate_training_data_with_extrapolation(self, 
                                                 curr_frame: Dict, 
                                                 prev_frame: Dict,
                                                 next_frame_gt: Dict,  
                                                 extrapolation_factor: float = 1.0) -> Dict:
        """
        ç”Ÿæˆå®Œæ•´çš„è®­ç»ƒæ•°æ® - ã€å¤–æ’ä¿®å¤ç‰ˆã€‘
        
        Args:
            curr_frame: å½“å‰å¸§(t)æ•°æ®
            prev_frame: å‰ä¸€å¸§(t-1)æ•°æ®
            next_frame_gt: çœŸå®çš„ä¸‹ä¸€å¸§(t+1)æ•°æ®ï¼Œä½œä¸ºGround Truth
            extrapolation_factor: å¤–æ¨ç³»æ•°(x)ã€‚é»˜è®¤ä¸º1.0ï¼Œè¡¨ç¤ºå¤–æ¨åˆ° t+1ã€‚
            
        Returns:
            training_data: åŒ…å«æ‰€æœ‰è®­ç»ƒæ•°æ®çš„å­—å…¸
        """
        print(f"   ğŸš€ ç”Ÿæˆå¤–æ’è®­ç»ƒæ•°æ®ï¼Œå¤–æ¨ç³»æ•°: {extrapolation_factor}")
        
        # 1. è®¡ç®— t-1 -> t çš„åŸºç¡€å±å¹•ç©ºé—´è¿åŠ¨çŸ¢é‡
        base_mv = self.compute_screen_space_mv(curr_frame, prev_frame)

        # 2. æ ¹æ®å¤–æ¨ç³»æ•°ï¼Œè®¡ç®—ç”¨äº t -> t+x çš„è¿åŠ¨çŸ¢é‡
        # æ ¸å¿ƒå‡è®¾ï¼šæ’å®šé€Ÿåº¦æ¨¡å‹
        extrapolated_mv = base_mv * extrapolation_factor
        
        if self.debug:
            base_magnitude = np.sqrt(base_mv[0]**2 + base_mv[1]**2)
            extrap_magnitude = np.sqrt(extrapolated_mv[0]**2 + extrapolated_mv[1]**2)
            print(f"   åŸºç¡€MVå¹…å€¼: å¹³å‡={base_magnitude.mean():.2f}px, æœ€å¤§={base_magnitude.max():.2f}px")
            print(f"   å¤–æ¨MVå¹…å€¼: å¹³å‡={extrap_magnitude.mean():.2f}px, æœ€å¤§={extrap_magnitude.max():.2f}px")

        # 3. å‰å‘æŠ•å½±å½“å‰å¸§(t)æ¥ç”Ÿæˆå¤–æ¨å¸§(t+x)
        # è¿™é‡Œéœ€è¦å¤–éƒ¨æä¾›forward_warp_frameå‡½æ•°
        # warped_image, coverage_mask = self.forward_warp_frame(
        #     curr_frame,       # æ³¨æ„ï¼æºå›¾åƒæ˜¯å½“å‰å¸§(t)
        #     extrapolated_mv   # ä½¿ç”¨å¤–æ¨åçš„è¿åŠ¨çŸ¢é‡
        # )
        
        # æš‚æ—¶è¿”å›å…³é”®æ•°æ®ï¼Œå®é™…é›†æˆæ—¶éœ€è¦è¿æ¥åˆ°å®Œæ•´çš„warpå’Œæ£€æµ‹æµç¨‹
        return {
            'base_mv': base_mv,
            'extrapolated_mv': extrapolated_mv, 
            'extrapolation_factor': extrapolation_factor,
            'target_frame_gt': next_frame_gt['reference'] if 'reference' in next_frame_gt else None
        }
    
    def create_integration_patches(self) -> Dict[str, str]:
        """åˆ›å»ºé›†æˆè¡¥ä¸ä»£ç """
        
        # è¡¥ä¸1: æ·»åŠ åˆ°UnifiedNoiseBasePreprocessorç±»ä¸­çš„screen_space_projectionæ–¹æ³•
        screen_projection_patch = '''
    @staticmethod
    def screen_space_projection(world_pos: np.ndarray, 
                               view_proj_matrix: np.ndarray, 
                               height: int, 
                               width: int) -> np.ndarray:
        """
        å°†ä¸–ç•Œç©ºé—´ä½ç½®æŠ•å½±åˆ°å±å¹•ç©ºé—´åƒç´ åæ ‡
        
        Args:
            world_pos: ä¸–ç•Œç©ºé—´åæ ‡ (3, H, W)
            view_proj_matrix: è§†å›¾æŠ•å½±çŸ©é˜µ (4, 4)
            height, width: å›¾åƒå°ºå¯¸
            
        Returns:
            å±å¹•ç©ºé—´åæ ‡ (2, H, W)ï¼Œå•ä½ä¸ºåƒç´ 
        """
        C, H, W = world_pos.shape
        if C != 3:
            raise ValueError(f"world_pos å¿…é¡»æ˜¯3é€šé“ï¼Œå®é™…ä¸º {C}")

        # æ‰©å±•ä¸ºé½æ¬¡åæ ‡
        ones = np.ones((1, H, W), dtype=world_pos.dtype)
        homogeneous_pos = np.concatenate([world_pos, ones], axis=0)

        # çŸ©é˜µæŠ•å½±
        projected_pos = np.einsum('ij,jhw->ihw', view_proj_matrix, homogeneous_pos)
        
        # é€è§†é™¤æ³•
        w = projected_pos[3:4, :, :]
        w = np.where(w == 0, 1e-8, w)
        ndc_pos = projected_pos[:2, :, :] / w

        # NDCåˆ°å±å¹•åæ ‡è½¬æ¢
        screen_pos_x = (ndc_pos[0] * 0.5 + 0.5) * width
        screen_pos_y = (-ndc_pos[1] * 0.5 + 0.5) * height

        return np.stack([screen_pos_x, screen_pos_y], axis=0)
'''

        # è¡¥ä¸2: æ›¿æ¢_process_motion_dataæ–¹æ³•
        process_motion_patch = '''
    def _process_motion_data(self, motion_data) -> np.ndarray:
        """å¤„ç†ä¸–ç•Œç©ºé—´è¿åŠ¨çŸ¢é‡æ•°æ®ï¼ˆä»…åŠ è½½å’Œèšåˆï¼‰"""
        motion = np.array(motion_data)
        
        # å¤šé‡‡æ ·èšåˆ
        if motion.ndim == 4:
            motion = motion.mean(axis=-1)
        
        # ç¡®ä¿æ ¼å¼ä¸ºCHW
        if motion.shape[-1] == 3:
            motion = motion.transpose(2, 0, 1)

        print(f"   å·²åŠ è½½ä¸–ç•Œç©ºé—´motionæ•°æ®: {motion.shape}")
        return motion.astype(np.float32)
'''

        # è¡¥ä¸3: æ–°å¢_compute_screen_space_mvæ–¹æ³•
        compute_mv_patch = '''
    def _compute_screen_space_mv(self, curr_frame: Dict, prev_frame: Dict) -> np.ndarray:
        """
        ã€æ ¸å¿ƒä¿®å¤ã€‘è®¡ç®—å±å¹•ç©ºé—´è¿åŠ¨çŸ¢é‡
        """
        pos_t = curr_frame['position']
        motion_t = curr_frame['motion'][:3]
        
        # è·å–æŠ•å½±çŸ©é˜µ
        vp_mat_t = curr_frame.get('camera_params', {}).get('view_proj_mat')
        vp_mat_prev = prev_frame.get('camera_params', {}).get('view_proj_mat')
        
        if vp_mat_t is None or vp_mat_prev is None:
            print("   âš ï¸ ç¼ºå°‘æŠ•å½±çŸ©é˜µï¼Œä½¿ç”¨é»˜è®¤çŸ©é˜µ")
            default_proj = self._create_default_projection_matrix()
            vp_mat_t = vp_mat_t or default_proj
            vp_mat_prev = vp_mat_prev or default_proj

        H, W = pos_t.shape[1], pos_t.shape[2]

        # è®¡ç®—å‰ä¸€å¸§ä¸–ç•Œåæ ‡
        pos_prev = pos_t - motion_t

        # æŠ•å½±åˆ°å±å¹•ç©ºé—´
        screen_pos_t = self.screen_space_projection(pos_t, vp_mat_t, H, W)
        screen_pos_prev = self.screen_space_projection(pos_prev, vp_mat_prev, H, W)
        
        # è®¡ç®—å±å¹•ç©ºé—´è¿åŠ¨çŸ¢é‡
        screen_space_mv = screen_pos_prev - screen_pos_t
        
        # è°ƒè¯•ä¿¡æ¯
        mv_magnitude = np.sqrt(screen_space_mv[0]**2 + screen_space_mv[1]**2)
        print(f"   âœ… å±å¹•ç©ºé—´MV: å¹³å‡={mv_magnitude.mean():.2f}px, æœ€å¤§={mv_magnitude.max():.2f}px")
        
        return screen_space_mv.astype(np.float32)
'''

        # è¡¥ä¸4: ä¿®æ”¹generate_training_dataæ–¹æ³•
        generate_training_patch = '''
    def generate_training_data(self, 
                             curr_frame: Dict, 
                             prev_frame: Dict,
                             next_frame_gt: Optional[Dict] = None,
                             extrapolation_factor: float = 1.0) -> Dict:
        """
        ç”Ÿæˆå®Œæ•´çš„è®­ç»ƒæ•°æ® - ã€å¤–æ’ä¿®å¤ç‰ˆã€‘
        """
        # 1. è®¡ç®—æ­£ç¡®çš„å±å¹•ç©ºé—´è¿åŠ¨çŸ¢é‡
        base_mv = self._compute_screen_space_mv(curr_frame, prev_frame)
        
        # 2. åº”ç”¨å¤–æ¨ç³»æ•°
        extrapolated_mv = base_mv * extrapolation_factor
        
        # 3. å‰å‘æŠ•å½±ï¼ˆä½¿ç”¨æ­£ç¡®çš„MVï¼‰
        if next_frame_gt is not None:
            # å¤–æ’æ¨¡å¼ï¼šä»å½“å‰å¸§å¤–æ¨åˆ°ä¸‹ä¸€å¸§
            warped_image, coverage_mask = self.forward_warp_frame(
                curr_frame, extrapolated_mv
            )
            target_rgb = next_frame_gt['reference']
        else:
            # é‡æŠ•å½±æ¨¡å¼ï¼šä»å‰ä¸€å¸§æŠ•å½±åˆ°å½“å‰å¸§
            warped_image, coverage_mask = self.forward_warp_frame(
                prev_frame, base_mv
            )
            target_rgb = curr_frame['reference']
        
        # 4. ç©ºæ´å’Œé®æŒ¡æ£€æµ‹
        masks = self.detect_holes_and_occlusion(
            warped_image, target_rgb, coverage_mask, curr_frame, prev_frame
        )
        
        # 5. æ®‹å·®è¿åŠ¨çŸ¢é‡è®¡ç®—
        if next_frame_gt is not None:
            # è®¡ç®—çœŸå®çš„t->t+1è¿åŠ¨çŸ¢é‡å¹¶è®¡ç®—æ®‹å·®
            gt_mv = self._compute_screen_space_mv(next_frame_gt, curr_frame)
            residual_mv = gt_mv - extrapolated_mv
        else:
            residual_mv = self._compute_residual_motion_vectors(
                warped_image, target_rgb, base_mv, masks['holes']
            )
        
        # 6. ç»„è£…è®­ç»ƒæ•°æ®
        training_sample = np.concatenate([
            target_rgb,                         # ç›®æ ‡RGB (3)
            masks['holes'][np.newaxis],         # å‡ ä½•ç©ºæ´æ©ç  (1)
            masks['occlusion'][np.newaxis],     # è¯­ä¹‰é®æŒ¡æ©ç  (1)
            residual_mv                         # æ®‹å·®è¿åŠ¨çŸ¢é‡ (2)
        ], axis=0)
        
        return {
            'target_rgb': target_rgb,
            'warped_image': warped_image,
            'coverage_mask': coverage_mask,
            'hole_mask': masks['holes'],
            'occlusion_mask': masks['occlusion'],
            'residual_mv': residual_mv,
            'training_sample': training_sample,
            'base_mv': base_mv,
            'extrapolated_mv': extrapolated_mv
        }
'''
        
        return {
            'screen_projection_patch': screen_projection_patch,
            'process_motion_patch': process_motion_patch,
            'compute_mv_patch': compute_mv_patch,
            'generate_training_patch': generate_training_patch
        }

def create_default_projection_matrix() -> np.ndarray:
    """åˆ›å»ºé»˜è®¤é€è§†æŠ•å½±çŸ©é˜µçš„ç‹¬ç«‹å‡½æ•°"""
    fov = np.pi / 3  # 60åº¦
    aspect = 16.0 / 9.0
    near, far = 0.1, 100.0
    
    f = 1.0 / np.tan(fov / 2)
    
    projection = np.array([
        [f / aspect, 0, 0, 0],
        [0, f, 0, 0], 
        [0, 0, (far + near) / (near - far), (2 * far * near) / (near - far)],
        [0, 0, -1, 0]
    ], dtype=np.float32)
    
    return projection

# æµ‹è¯•å‡½æ•°
def test_motion_vector_calculation():
    """æµ‹è¯•è¿åŠ¨çŸ¢é‡è®¡ç®—"""
    print("ğŸ§ª æµ‹è¯•è¿åŠ¨çŸ¢é‡ä¿®å¤")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    H, W = 1080, 1920
    
    # æ¨¡æ‹Ÿå½“å‰å¸§æ•°æ®
    curr_frame = {
        'position': np.random.randn(3, H, W).astype(np.float32) * 10,
        'motion': np.random.randn(3, H, W).astype(np.float32) * 0.1,
        'camera_params': {
            'view_proj_mat': create_default_projection_matrix()
        }
    }
    
    # æ¨¡æ‹Ÿå‰ä¸€å¸§æ•°æ®
    prev_frame = {
        'position': curr_frame['position'] - curr_frame['motion'],
        'motion': curr_frame['motion'],
        'camera_params': {
            'view_proj_mat': create_default_projection_matrix()
        }
    }
    
    # æµ‹è¯•è¿åŠ¨çŸ¢é‡è®¡ç®—
    fixer = MotionVectorFixer(debug=True)
    screen_mv = fixer.compute_screen_space_mv(curr_frame, prev_frame)
    
    print(f"âœ… æµ‹è¯•å®Œæˆ")
    print(f"   å±å¹•ç©ºé—´MVå½¢çŠ¶: {screen_mv.shape}")
    print(f"   MVæ•°å€¼èŒƒå›´: [{screen_mv.min():.3f}, {screen_mv.max():.3f}]")
    
    return screen_mv

if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    test_mv = test_motion_vector_calculation()
    
    # åˆ›å»ºé›†æˆè¡¥ä¸
    fixer = MotionVectorFixer()
    patches = fixer.create_integration_patches()
    
    print("\nğŸ“‹ å·²ç”Ÿæˆé›†æˆè¡¥ä¸ä»£ç ")
    print("   å¯ä»¥ä½¿ç”¨è¿™äº›è¡¥ä¸æ¥ä¿®æ”¹unified_noisebase_preprocessor.py")